#!/usr/local/bin/python3
#
# SSST
#
# A simple static site tool to maintain websites based on markdown and pandoc
#
# Author: Jaap-henk Hoepman (jhh@xs4all.nl)

import argparse, sys, os, re, tempfile, subprocess, shutil, datetime, yaml, filecmp, stat
from pathlib import Path
from io import StringIO
from zoneinfo import ZoneInfo

# ---
# DelayedTextfile
#
# Create a textfile, whose contents is only actually written to external
# storage when closing the file and the new contents are different from
# the old contents (or when the file does not exist yet)
# ---

class DelayedTextfile(StringIO):

    # create a Delayedtextfile for external file fname,
    # to be written on file close
    def __init__(self,fname):
        super().__init__()
        self.path = Path(fname)

    # return true if the file was actually written, false otherwise    
    def close(self):
        newcontents = self.getvalue()
        if self.path.exists():
            with open(self.path,'r') as f:
                oldcontents = f.read()
                if oldcontents == newcontents:
                    log(5,'File not changed ' + str(self.path))
                    return False
        with open(self.path,'w') as f:
            f.write(newcontents)
            log(5,'File changed ' + str(self.path))
            return True

    # Allow with DelayedTextfile(name) as f: constructions
    # where file is automatically closed when with statement ends
    def __exit__(self, type, value, tb):
        self.close()

# ---
# CONSTANTS / PARAMETERS
# ---

# All items processed by SSST are *relative* paths to posts or pages, relative
# to the src_root and dest_root prefixes specified on the commandline
# and 'declared' as global variables here

# Path to the root of the source files tree
src_root = Path('./src')

# Path to the root of the destination files tree
dest_root = Path('./public')

# Root where the site is hosted
# (relative adressing, making the whole site relocatable, is the default)
hosting_root = ''

# Path to directory with templates
templates = Path('./templates')

# Verbosity
loglevel = -1

# Force remaking all files
force = False 

# Overwrite files even when contents are unchanged
overwrite = False 

# Use pandoc --gladtex to process LaTeX equations
use_gladtex = False

# Abort after warning
pedantic = False 

# Do not process simple LaTeX equations (pass them through in italics)
keep_simple_eqs = False

# Lenght of summaries (including YAML header)
summary_length = 20

# logfile name
logfile = None 

# upload script file name
uploadscript = None 

# upload command, can containt the following modifiers
# {path} : replaced with the destination path of the changed item (without the
#    destination prefix specified by -d
# {src} : full path name of the changed file that needs to be uploaded
uploadcommand = ''

# ---
# OTHER GLOBAL VARIABLES
# ---

# List of all files in dest_root that changed in the process
# (relative to dest_root)
dest_root_changes = []

# Dictionary of all tags, listing for each tag the relative pathnames for the posts with those tags
# (filled by process_posts -> process_post_metadata) 
posts_with_tag = {}

# Dictionary of all categories, listing for each category the relative pathnames for the posts with those categories
# (filled by process_posts -> process_post_metadata) 
posts_with_cat = {}

# Dictionary of all posts (indexed using their relative pathnames), containing
# the metadata as tuples (date,title,template)
# (filled by process_posts -> process_post_metadata) 
post_metadata = {}

# List of all relative pathnames for posts, sorted by date (most recent first)
# (with the help of process_posts)
all_posts = []

# ---
# ERRORS / LOGGING
# ---

# Outpur the error message, and log it when required. Then exit
# - msg: the error message
def error(msg: str):
    m = 'ssst: error: %s\n' % str(msg)
    sys.stderr.write(m)
    if logfile != None:
        logfile.write(m)
    sys.exit (1)

# Output the warning message, and write it to the log file if it was specified.
# Exit when pedantic processing was specified.
# - msg: the warning message
def warning(msg: str):
    m = 'ssst: warning: %s\n' % str(msg)
    sys.stderr.write(m)
    if logfile != None:
        logfile.write(m)
    if pedantic:
        sys.exit (1)

# Output a log message when its level is lower than the specifief verbosity
# on the command line. Always write the message to the log file if it was
# specified 
# - level: the log-level for this message
# - msg: the log message
def log (level: int, msg: str):
    if level == 0:
        m = '# '
    else:
        m = '-' * level + ' '
    m = m + str(msg) + '\n'
    if (level <= loglevel):
        sys.stdout.write(m)
    if logfile != None:
        logfile.write(m)


# ---
# SHELL
# ---

# Run a shell command. Abort and print the error message when it fails
# - command
def run_shell_command(command: str):
    log(4,'Running shell command ' + command)
    result = subprocess.run(command,text=True,shell=True,capture_output=True)
    if result.returncode != 0:
        error(result.stderr)
    
# ---
# YAML
# ---

# Return the dictionary of entries found in the YAML block at the start of
# a file
# - src: full path to the file
def get_yaml_dict(src: Path):
    try:
        with open(src,'r') as f:
            yamls = yaml.load_all(f,Loader=yaml.SafeLoader)
            for y in yamls:
                return y
    except:
        warning('Invalid YAML block in ' + str(src))
        return {}

# ---
# FOLDER FUNCTIONS
# ---

# Return the full source folder containing item
# - item: relative path to the item
def src_folder_of(item: Path) -> Path:
    return src_root / item.parent # full source folder of item

# Return the full source folder pathname pointing to item
# - item: relative path to the item
def src_path_of(item: Path) -> Path:
    return src_root / item

# Return the relative path (wrt src_root) for the full path leading to an item
# -item: full path in source tree to item
def item_relpath(item: Path) -> Path:
    return item.relative_to(src_root)

# Return the full destination folder to store the result for item
# - item: relative path to the item
def dest_folder_of(item: Path) -> Path:
    return dest_root / item.parent # full source folder of item

# Return the full destination path for the html file to be generated
# for the item
# - item: relative path to the item
def dest_path_of (item: Path) -> Path:
    dest = dest_root / item
    return dest.with_suffix('.html')

# Return an iterator that iterates over all files in the tree rooted at
# src_root with the specified suffix
# - suffix: suffix of files to return, including "."
def src_files_with_suffix(suffix: str):
    return Path(src_root).glob('**/*' + suffix)

# Determine list of Paths containing the comments (empty if none) for the item
# I.e. all files matching <src_root> / <parent_folder> / *.mdc
# - item: relative path to the item
def commentfiles_for (item: Path):
    return (src_root / item.parent).glob('*.mdc')

# Suffix of media files that will automatically be copied from the source
# folder to the destination folder
media_types = ['jpg','gif','pdf','png']

# Determine list of Paths containing the mediafiles (empty if none) for the item
# I.e. all files matching <src_root> / <parent_folder> / <mediatypes>
# - item: relative path to the item
def mediafiles_for (item: Path):
    res = []
    for tp in media_types:
        res = res + list((src_root / item.parent).glob('*.' + tp))
    return res

# Return path pointing to the root for the current item on the hosting site.
# This is either a relative path back to the root from the path containing the
# item, or simply hosting_root if set to non-empty on the command line 
# - item: relative path to item
def item_root (item: Path) -> str: 
    if hosting_root != '':
        return hosting_root
    else:
        item = str(item)
        log(5,'Computing root for ' + item)
        res = ''
        for c in item:
            if c == '/':
                res = res + '../'
        # remove trailing backslash (does nothing when res == '')
        res = res[:-1]
        if res == '':
            res = '.'
        log(5,'Computed root ' + res)    
        return res

# Determine the link pointing to the item on the hosting site
# relative to context, i.e.
# the generated link works if is included in the page with path context
# - item: relative path to item
# - context: relative path to page that will contain the link to item 
def item_url (item: Path, context: Path) -> str:
    item = item.with_suffix('.html')
    return str(item_root(context) / item)

# Return the full path for a template, throw error if it does not exist
# - name: template file name to be found in the template directory  
def get_template(name: Path):
    template = templates / name        
    if not template.exists():
        error('Template %s not found' % template)
    return template

# Return the full path for a template, throw error if it does not exist
# - name: template file name to be found in the template directory
# - default: default template file to be used if name does not exist
def get_template_or_default(name: Path, default: Path):
    template = templates / name        
    if not template.exists():
        return get_template(default)
    return template

# ---
# OTHER FUNCTIONS
# ---

# Replace the contents of a file with new contents, but only change the file
# and its metadata when the new contents are different from the current
# contents.
# - current: file whose contents must be updated 
# - update: file containing new content
def replace_when_changed (current: Path, update: Path):
    if (not current.exists()) \
       or overwrite \
       or (not filecmp.cmp(current,update,shallow=False)): 
        log(5,'File changed ' + str(current))
        dest_root_changes.append(current.relative_to(dest_root))
        update.rename(current)
    else:
        log(5,'File not changed ' + str(current))
        update.unlink() # remove file


# Parse a date if it is a string, return unchanced otherwise.
# (Only parses wordpress generated dates like 'Sun, 19 Apr 2020 08:42:00 +0000'
# that the YAML parser fails to parse)
# - s: string to parse if it isn't a date
def parse_date (s) -> datetime.date:
    if type(s) is str:
        time = datetime.datetime.strptime(s,'%a, %d %b %Y %H:%M:%S %z')
        return time.date()
    else:
        return s

# Convert a date to something like 'April 03, 2013'
# - date: datetime object to be converted to a string
def normalize_date(date: datetime.date) -> str:
    # we do NOT want zero-padding on days
    return date.strftime('%B ') + str(date.day) + ', ' + str(date.year)

# Minimum amount of milliseconds difference between destination and source 
# modification time before an item will be considered older (and hence a
# workitem)

grace_period = 10

# Check whether dest is older than src
# - dest
# - src
def is_older(dest,src: Path):
    dtime = os.stat(dest)[stat.ST_MTIME]
    stime = os.stat(src)[stat.ST_MTIME]
    return dtime < (stime - grace_period)

# ---
# PROCESS POSTS
# ---

# Process post metadata and store it in post_metadata[].
# Determine the categories and the tags assiociated with a post 
# and add its  pathname to the necessary categories and tag dictionaries
# - post : full pathname to the post to process
def process_post_metadata(post: Path):
    global posts_with_tag
    global posts_with_cat
    global post_metadata
    log(2,'Processing categories and tags for ' + str(post))
    yaml = get_yaml_dict(post)
    post_relpath = item_relpath(post)
    if 'categories' in yaml:
        cats = yaml['categories']
        log(3,'Categories: ' + str(cats))
        # careful: categories may be a single string
        if type(cats) is str:
            cats = [ cats ]
        for cat in cats:
            # make sure categoreis are stored in all lowercase
            # (e.g. ios and iOS would otherwise be considered different tags
            # but written to the same external tag file if the filesystem
            # is case-insensitive; one overwrting the other!!!
            cat = cat.lower()
            if cat not in posts_with_cat:
                posts_with_cat[cat] = []
            posts_with_cat[cat].append(post_relpath)
    if 'keywords' in yaml:
        tags = yaml['keywords']
        log(3,'Tags: ' + str(tags))
        # careful: tags may be a single string
        if type(tags) is str:
            tags = [ tags ]
        for tag in tags:
            if not tag.isupper():
                tag = tag.capitalize()
            if tag not in posts_with_tag:
                posts_with_tag[tag] = []
            posts_with_tag[tag].append(post_relpath)
    if 'title' in yaml:
        title = yaml['title']
    else:
        title = ''
        warning('Title expected but not found in ' + str(post) + \
                '. Using empty title.')
    if 'date' in yaml: 
        tmp = yaml['date']
        date = parse_date(tmp)
        dateprefix = date.strftime('%Y/%m/%d')
        poststr = str(item_relpath(post))
        if not poststr.startswith(dateprefix):
            warning('Date ' + dateprefix + ' in ' + str(post) + \
                    ' does not correspond with its path.')
    else:
        error('Date expected but not found in ' + str(post))
    if 'template' in yaml:
        template = yaml['template']
    else:
        template = ''
    post_metadata[post_relpath] = (date,title,template)

# Regular expression matching a path for a post (i.e. one that starts with a
# four digit year)
# FIXME: this is incomplete
post_re = re.compile('[0-9][0-9][0-9][0-9].*')
    
# Determine whether the item is a post (or a page)
# - item: relative path to item to determine type of
def is_post(item: Path) -> bool:
    return (post_re.match(str(item)) != None) and (item.suffix == '.md')


# Collect all metadata about all posts (like tags and categories, but also
# title and date for all blog entries) in the source tree. This initializes
# the following global variables:
# posts_with_cat, posts_with_tag, post_metadata and all_posts
def process_posts ():
    log(0,'Processing posts.')
    # Find all posts (they have suffix .md) and process their metadata
    for item in src_files_with_suffix('.md'):
        if is_post(item_relpath(item)):
            process_post_metadata(item)
    # sort all posts by date and store the list of relative paths in all_posts
    for post in post_metadata:
        all_posts.append(post)
    # sort in decending order
    all_posts.sort(reverse=True,key=lambda entry: post_metadata[entry][0])
    

# ---
# LaTeX EQUATIONS
# ---

# number of matches so far
eq_match_count = 0
# folder containing the currently processed file
basefolder = ''
# dictonary of processed equations, containing for each equation the
# name of the SVG file containing its image;
# reset for every file
equations = {}

# Return the filename of the file to store the SVG for the i-th equation in
def get_svg_file_name(i):
    return 'index.' + str(i) + '.svg'

# Return the full pathname of the file to store the SVG for the i-th equation in
def get_svg_path_name(i):
    return basefolder / get_svg_file_name(i)

# Turn a latex expression into an SVG graphic and write it to file.
# The filename to use is determined by the global variable basefolder
# (derived from the name of the file containing the equations) and
# number of processed equations for that file
# - latex_expr: latex expression (without surrounding $$)
# - returns: the name of the SVG file in which the graphic is stored
#   (relative to basename)
def process_eq(latex_expr):
    global eq_match_count
    eq_match_count = eq_match_count + 1
    # create the SVG output name
    svg_file_name = get_svg_file_name(eq_match_count) 
    svg_path_name = get_svg_path_name(eq_match_count) 
    svg_tmp = svg_path_name.with_suffix('.tmp')
    log(4,'Storing equation in ' + str(svg_path_name))
    # creata a temorary direcotry to work in; will be deleted when with closes
    with tempfile.TemporaryDirectory() as tmpdir:
        tmp_tex = tmpdir + '/htsv.tex'    
        tmp_pdf = tmpdir + '/htsv.pdf'
        # read the latex template and store it as a string in t
        eq_template = get_template('ssst-eq.tex')
        with open(eq_template,'r') as f:
            latex_template = f.read()
        # insert latex_expr at $$ in the template
        s = latex_template.split('$$')
        latex_document = s[0] + '$'+ latex_expr + '$' + s[1]
        # write the result to a tex file in the temporary directory
        with open(tmp_tex,'w') as w:
            w.write(latex_document)
        # create the pdf (in the temporary directory) from the latexfile
        run_shell_command('pdflatex -output-directory ' + \
                             tmpdir + ' ' + tmp_tex)
        # create the SVG from the pdf
        run_shell_command('pdf2svg ' + tmp_pdf + ' ' + str(svg_tmp))
        # replace destination only when real changes occured
        replace_when_changed(svg_path_name,svg_tmp)    
    return svg_file_name

# Match &lt;
lessthan_re = re.compile(r'(&lt;)')

# Match &gt;
greaterthan_re = re.compile(r'(&gt;)')

# Replace any occurance of '&lt;' on the input with '<'
# (because the --gladtex option of pandoc turns < into &lt;)
def sanitize_eq (expr):
    ret = lessthan_re.sub('<',expr,count=0)
    ret = greaterthan_re.sub('>',ret,count=0)    
    log(5,'Sanitized '+ expr + ' to ' + ret)
    return ret

# Match a simple expression, that can be passed without processing
simple_expr_re =  re.compile(r'([A-Za-z0-9])+')

# Process currently matched latex equation to create an SVG for it
# return the replacement text for the currently matched latex equation
# (ie an <img> tag pointing the SVG file generated for it)
# If the exact same equation was already made, immediately return the
# <img> tag pointing to the SVG file previously made.
# - matchobj: object found to match the equation regular expression 
def substitute_eq (matchobj):
    if matchobj:
        # get the matched latex expression string
        match = matchobj.group(1)
        # sanitize expressions
        latex_expr = sanitize_eq(match)
        log(4,'Processing equation ' + latex_expr)
        # do not process simple LaTex equations, when so instructed
        if keep_simple_eqs and simple_expr_re.fullmatch(latex_expr):
            return '<em>' + latex_expr + '</em>'
        if latex_expr in equations:
            log(5,'Already made!')
            svg_file_name = equations[latex_expr]
        else:
            svg_file_name = process_eq(latex_expr)
            equations[latex_expr]=svg_file_name
        # return the <img> tag that points to the generated SVG
        return '<img class="latexeq"' \
            + ' src="' + svg_file_name + '"' \
            + ' alt="' + latex_expr + '">'
    
# Match <eq ...>LATEXEXPRESSION</eq> generated by pandoc -gladtex
# (use .*? for non-greedy matching, to ensure the first </eq> found
# will close the regulear expression)
gladtex_eq_re = re.compile(r'<eq[^>]*>(.*?)</eq>',re.DOTALL)

# Match <span class="math inline">$LATEXEXPRESSION$</span> generated by
# pandoc without any math processing option.
# (use .*? for non-greedy matching, to ensure the first $</span> found
# will close the regulear expression)
plain_eq_re = re.compile(r'<span class="math inline">\$(.*?)\$</span>',re.DOTALL)

# Process any equations in the item (determined using the regular
# expression eq_re)
# - fname: full path of item to be processed (note: the parent is the folder
#      in which any equation images are stored)
def process_equations (fname):
    global eq_match_count
    global basefolder
    global equations
    log(2,'Processing equations.')
    eq_match_count = 0
    equations = {}
    basefolder = fname.parent
    # open the file for reading
    with open(fname,'r') as f:
        # read the file as one string
        orig_html = f.read()
    # process and substitute each occurence of a latex equation
    if use_gladtex:
        processed_html = gladtex_eq_re.sub(substitute_eq, orig_html, count=0)
    else:
        processed_html = plain_eq_re.sub(substitute_eq, orig_html, count=0)
    # write the result back to the file
    with open(fname,'w') as f:
        f.write(processed_html)
    # remove any SVG fles containing equations from previous runs
    eq_match_count = eq_match_count + 1
    svg_path_name = get_svg_path_name(eq_match_count) 
    while svg_path_name.exists():
        svg_path_name.unlink() # remove file
        eq_match_count = eq_match_count + 1
        svg_path_name = get_svg_path_name(eq_match_count) 
    
# ---
# MAKE ITEM
# ---

# return the path to next post (more recent) following item; '' if none
# (this uses the fact that all_posts is sorted by date in descending order)
def next_post(item):
    if item in all_posts:
        pos = all_posts.index(item)
        if pos-1 >= 0:
            return all_posts[pos-1]
    return ''

# return the path to previous post (less recent) following item; '' if none
# (this uses the fact that all_posts is sorted by date in descending order)
def prev_post(item):
    if item in all_posts:
        pos = all_posts.index(item)
        if pos+1 < len(all_posts):
            return all_posts[pos+1]
    return ''

# Make the html item from the md or sst source specified by item, merging
# in any comments, and using item_template as the template
def make_item(item,comments,item_template):
    # create destination folder if it doesnt exist yet
    os.makedirs(dest_folder_of(item), exist_ok=True)
    # blog-post path for reply links: strip the source prefix in front
    # and the index.md at the end; add comment.
    reply_path = str(item.parent) + '/comment.'
    # create metadata for next and prev links when relevant
    extra_metadata_str = ''
    next = next_post(item)
    if next != '':
        extra_metadata_str = ' --metadata next="' + item_url(next,item) + '"'
    prev = prev_post(item)
    if prev != '':
        extra_metadata_str = extra_metadata_str + \
            ' --metadata prev="' + item_url(prev,item) + '"'
    # add normalised date metadata for posts (archives and tag/category indices
    # do  not have this, but are treated as posts for the time being)
    if item in post_metadata:
        date = post_metadata[item][0]
        extra_metadata_str = extra_metadata_str + \
            ' --metadata date="' + normalize_date(date) + '"' 
    # build the command string
    src = src_path_of(item)
    dest = dest_path_of(item)
    if use_gladtex:
        gladtex_str = ' --gladtex '
    else:
        gladtex_str = '' 
    command = 'pandoc ' + gladtex_str + '-s -f markdown -t html --template=' + \
        str(item_template) + \
        ' -A '+ comments + \
        ' --metadata reply=' + reply_path + \
        ' --metadata root=' + str(item_root(item)) + \
        extra_metadata_str + \
        ' -o '+ str(dest) + ' ' + str(src)
    run_shell_command(command)
    # process any equations in the output
    process_equations(dest)
    # record that destination has changed
    dest_root_changes.append(dest.relative_to(dest_root))

    
# ---
# MAKE COMMENTS
# ---

# Assume all comment files have been converted to html and are stored in
# the temporary directory. 
# Recursively fold all coments at the current level (specified by path in
# the temporary directory)
# and return them as a string; try: <path>.1.html (and fold its children)
# then  <path>.2
def fold_comments(path):
    log(4,'Folding ' + str(path)) 
    folded = ''
    i = 1
    while True:
        prefix = path + '.' + str(i)
        filename = prefix + '.html'
        if not os.path.exists(filename):
            break
        with open(filename,'r') as f:
            folded = folded \
                + '<div class="comment">\n'\
                + f.read() \
                + fold_comments(prefix) \
                + '</div>\n'
        i = i + 1
    return folded
    
# Process the comments associated with the item. 
# - first convert individual comments to html in tmpdir
# - then fold them into one html file (keeping the hierarchical order)
# Stores the result in tmpdir/comments.html for further processing
# by make (this file is empty if there are no comments)
def make_comments(item,tmpdir):
    log(2,'Processing comments')
    commentfiles = commentfiles_for(item)
    comment_template = get_template('ssst-comment.html')
    # first convert all .mdc files in src_root to .html files in tmpdir
    for c in commentfiles:
        # filename to store html converted comment in (in tmpdir)
        outf = tmpdir + '/' + str(c.name)[:-3] + 'html'
        # blog-post path and comment depth indicator for reply links
        # strip the source prefix in front and mdc from the back
        reply_path = str(item_relpath(c))
        reply_path = reply_path[:-3]
        log(3,'Making comment HTML ' + outf)
        command = 'pandoc -f markdown'+ \
            ' --metadata title=None'  + \
            ' --metadata reply=' + reply_path + \
            ' --template=' + str(comment_template) + \
            ' -o ' + outf + ' ' + str(c)
        run_shell_command(command)
    # now recursively fold in comments created in tmpdir
    comments = fold_comments(tmpdir + '/comment')
    log(5,comments)
    # and store the comments in comments.html in the tmpdir
    with open(tmpdir + '/comments.html','w') as f:
        f.write(comments)

# ---
# DUMP CATS AND TAGS ; ARCHIVES
# ---

# Write a link (entry_url) describing an entry (using its title and date)
# to the output stream outf
def dump_entry(entry,outname,outf):
    (date,title,template) = post_metadata[entry]
    datestr = normalize_date(date)
    outf.write('- [' + title + '](' + item_url(entry,outname) + \
               ') (' + datestr + ')\n')    
    
# Create a markdown document into out_name (relative path)
# (with title as title and label as subtitle)
# containing a list of links to all posts in filelist
# (assumed to be sorted by date)
def dump_entries(out_name,type,label,filelist):
    # sort the list of pathnames by date, most recent first
    # (using the fact that the path encodes the file date)
    filelist.sort(reverse=True)
    os.makedirs(src_folder_of(out_name),exist_ok=True)
    log(3,'Dumping to ' + str(out_name))
    with DelayedTextfile(src_path_of(out_name)) as outf:
        outf.write('---\n')
        outf.write("title: '" + label + "'\n")
        outf.write("subtitle: '" + type + "'\n")        
        outf.write('---\n')        
        for entry in filelist:
            dump_entry(entry,out_name,outf)
                
# Dump the category and tag dictionaries to the necessary pages in the
# ./category/.. and ./tag/... directories in the src_root (SOURCE!) tree;
# These are markdown files (with suffix .sst) that will subsequently be
# converted to html through the make routine
# Files whose contents would not change will not be touched; this way
# amke will ignore them as the timestamp remains unchanged
def dump_cats_and_tags():
    log(0,'Dumping category and tag files.')
    categories_pathname = src_path_of('categories.sst')
    # DelayedTextfile will be written to disk when with closes
    # (and contents really chagned)
    with DelayedTextfile(categories_pathname) as catsf:        
        catsf.write('---\n')
        catsf.write("title: 'Categories'\n")
        catsf.write('---\n\n')
        for cat in sorted(list(posts_with_cat)):
            cat_pathname = Path('category/' + cat + '/index.sst')
            catsf.write('- [' + cat + '](' + item_url(cat_pathname,'.') + ')\n')
            dump_entries(cat_pathname,'Category',cat,posts_with_cat[cat])
    tags_pathname = src_path_of('tags.sst')
    # DelayedTextfile will be written to disk when with closes
    # (and contents really chagned)
    with DelayedTextfile(tags_pathname) as tagsf:        
        tagsf.write('---\n')
        tagsf.write("title: 'Tags'\n")
        tagsf.write('---\n\n')
        firstchar = ''
        for tag in sorted(list(posts_with_tag), key=str.lower):
            tag_pathname = Path('tag/' + tag.lower() + '/index.sst')
            # output a header for each alphabetical change
            if firstchar != tag[0]:
                firstchar = tag[0]
                tagsf.write('\n\n## ' + firstchar.upper() + '\n\n') 
            tagsf.write('- [' + tag + '](' + item_url(tag_pathname,'.') + ')\n')
            dump_entries(tag_pathname,'Tag',tag,posts_with_tag[tag])

# return a sort key for an entry such that entries are sorted
# by decreasing year, but increasing month and day
def archive_sort_key(entry):
    date = post_metadata[entry][0]
    return (-date.year,date.month,date.day)

# Create archive files containing all posts for a particular month in 
# ./yyyy/mm/index.sst in the src_root (SOURCE!) tree;
# Also create a list of all known archives in src_root/archives.sst
# These are markdown files that will subsequently be converted to html
# through the make routine.
# Files whose contents would not change will not be touched; this way
# amke will ignore them as the timestamp remains unchanged
def dump_archives():
    log(0,'Dumping archives.')
    last_year_archived = 0
    # dictionary with DelayedTextFile descriptors, indexed by archivepath
    archive_paths = {}
    # sort the posts by *decreasing* year, but increasing month/day within
    # a year; this ensures that the main archive file lists the most
    # recent year first
    archive_posts = all_posts.copy()
    archive_posts.sort(key=archive_sort_key)
    # create the root archive file (as a DelayedTextfile
    archive = src_path_of('archives.sst')
    log(3,'Writing all archives to ' + str(archive))
    # DelayedTextfile will be written to disk when with closes
    # (and contents really chagned)
    with DelayedTextfile(archive) as archivef:        
        archivef.write('---\n')
        archivef.write("title: 'Archives'\n")
#        archivef.write("template: 'ssst-archives.html'\n")
        archivef.write('---\n')
        # process all posts and put them in the appropriate archive
        # this creates another DelayedTextfile for every new archive
        for post in archive_posts:
            # construct the full and relative path for the archive this post
            # should appear in
            date = post_metadata[post][0]
            archive_year = date.year
            monthyear = date.strftime('%B %Y')
            archive_relative_path = date.strftime('%Y/%m/index.sst')
            archive_path = src_path_of(archive_relative_path)
            # if we are processing a new month
            if archive_path not in archive_paths:
                # write a header every time the year changes to the main
                # archive file (this assumes archive_posts is sorted)
                if last_year_archived != archive_year:
                    last_year_archived = archive_year
                    archivef.write('\n## ' + str(archive_year) + '\n')
                # write a link to this new archive to the main archive file
                archivef.write('- [' + monthyear +'](' + \
                               item_url(Path(archive_relative_path),\
                                        Path('archives.sst')) + \
                               ')\n' )
                # create a new delayed file for this monthly archive
                log(3,'Creating new monthly archive ' + str(archive_path))
                outf = DelayedTextfile(archive_path)
                archive_paths[archive_path] = outf
                # write the header for the new archive
                outf.write('---\n')
                outf.write("title: 'Archive " + monthyear + "'\n")        
                outf.write('---\n')
            # append the entry to the monthly archive
            outf = archive_paths[archive_path]
            dump_entry(post,archive_relative_path,outf)
    # close all archive files
    for archive_path in archive_paths:
        archive_paths[archive_path].close()
    
# ---    
# DETERMINE WORKLIST
# ---

# Determine whether the item (given src and dest directories in the global
# variables src_root and dest_root) should be remade: i.e. if the destination
# does not exist, or when it is older than the source file or any of its
# associated comments or media files (within a certain grace period)
# - item: path to item
# - returns: whether item is a workitem or not
def is_workitem(item: Path) -> bool:
    # check whether destination exists
    dest = dest_path_of(item)
    log(4,'Destination should be ' + str(dest))
    if not dest.exists():
        return True
    log(4,'Destination exists.')
    # check whether destination is older than source
    src = src_path_of(item)
    if is_older(dest,src):
        log(4,'Destination older than source.')
        return True
    # check whether destination is older than any comment or mediafile
    # in the source
    files = list(commentfiles_for(item)) + mediafiles_for(item)
    for file in files:
        log(4,'Additional file found: ' + str(file))
        if is_older(dest,file):
            log(4,'Destination older than comment or media item in source.')
            return True
    return False


# match all files with suffix in the tree rooted at src_root and return a
# list of all files that need to be (re)made
# - suffix: suffix of files to consider (.md or .sst)
# - returns: a list of paths to items that need to be made
def worklist_for (suffix: str):
    list = []
    # traverse all files with suffix in the subtree rooted at src
    # (This returns a 'list' of Paths (not strings))
    for src in src_files_with_suffix(suffix):
        log(2,'Considering: ' + str(src))
        # strip the src_root
        item = item_relpath(src)
        # add the item when -f given, or when a workitem
        if force or is_workitem(item):
            log(2,'Adding ' + str(src))
            list.append(item)
        else:
            log(2,'Ignoring ' +  str(src))
    return list

# Determine posts, pages and generated pages that need to be (re)made
# - returns: a list of paths to items that need to be made
def worklist ():
    log(0,'Determining files to process...')
    # determine all .md files that need to be made
    list = worklist_for('.md')
    # remove <src_root>/index.md which is (always) made separetely
    # through make_home() called later
    if Path('index.md') in list:
        list.remove(Path('index.md'))
    extended_list = list.copy()
    # insert any "neighbours" of posts in the worklist as well, to ensure
    # that their next and prev links are updated if necessary
    for item in list:
        if is_post(item):
            prev = prev_post(item)
            if (prev != '') and (prev not in extended_list):
                extended_list.append(prev)
            next = next_post(item)
            if (next != '') and (next not in extended_list):
                extended_list.append(next)
    # add all *.sst files that need to be made
    final_list = extended_list + worklist_for('.sst')
    log(0,str(len(final_list)) + ' files to process found.')
    return final_list

# ---
# MAKE
# ---

# Copy all media associated with an item, keeping their metadata.
# - item: path to item in the source tree
def copy_media(item: Path):
    log(2,'Copying mediafiles')
    dest_folder = dest_folder_of(item)
    for f in mediafiles_for(item):
        log(4,'Copying media file ' + str(f) + ' to ' + str(dest_folder))
        dest_root_changes.append(item_relpath(f))
        shutil.copy2(f,dest_folder) # keep metadata

# Remove all non html and non .svg files (not diretories) from a folder
# (Keep the html and svg files because that will be remade, and we want to check
# later whether the new version is the same as the old or not)
# (After making equations any remaining old equation files are removed)
# - folder: folder to clean
def clean_folder(folder: Path):
    log(2,'Cleaning destination ' + str(folder))
    for f in folder.glob('*'):
        if (not f.is_dir()) and (f.suffix != '.html') and (f.suffix != '.svg'):
            os.remove(f)

# Return the template to use to process post: default if none specified
# in the YAML header for item
def post_template(post,default):
    if post in post_metadata:
        template = post_metadata[post][2]
        if template != '':
            return get_template(template)
    return get_template(default)

# Return the template to use to process page: default if not found
def page_template(page,default):
    template = default
    if str(page) == 'archives.sst':
        template = 'ssst-archives.html'
    elif str(page) == 'tags.sst':
        template = 'ssst-tags.html'
    elif str(page) == 'categories.sst':
        template = 'ssst-categories.html'
    return get_template_or_default(template,default)

# Make the output (in dest_root) for the item (from src_root). The output
# replaces what is in the destination always (and dest_root_changes updated
# accordingly): the reason is that make is called if something in src is newer
# than the destination, so we have to make sure that the timestamp changes (even
# if the actual contents do not change) or else this item will be remade forever
#
# Media files are copied to the destination folder if new, changed, or newer
# than what is in the destination. Media files removed from the source are
# removed from the destination.
def make(item: Path):
    log(1,'Processing ' + str(src_path_of(item)))
    # Clean the destination folder (only for posts that are guaranteed
    # have separate folders!)
    dest_folder = dest_folder_of(item)
    if (is_post(item) and dest_folder.exists()):
        clean_folder(dest_folder)
    # creata a temorary directory to work in; will be deleted when with closes
    with tempfile.TemporaryDirectory() as tmpdir:
        # make comments, stores result in tmpdir/comments.html
        make_comments(item,tmpdir)
        if is_post(item):
            log(2,'Processing post')
            template = post_template(item,'ssst-post.html')
        else:
            log(2,'Processing page')
            template = page_template(item,'ssst-page.html')
        # make the item using the specified template, merging in comments
        # in tmpdir
        make_item(item,tmpdir + '/comments.html',template)
    copy_media(item)

# ---
# MAKE HOME
# ---

# Regular expression matching img src's and href's that point to a local
# media object (i.e. paths that start with './' and then do not contain
# further slashes). A match returns two groups: 1) the src/href prefix and
# 2) the path without the leading "."
# (FRAGILE CODE: OTHER ITEMS MAY ALSO START WITH ./ )
localpath_re = re.compile(r'(src="|href=")\.(/[^/ ]+)"')

# Replacement to use when replacing local paths 
localpath_replacement = '' 

# filter replacing matched localpath with replacement text
# (using localpath_replacement)
# - matchobj: current match
def substitute_localpath(matchobj) -> str:
    if matchobj:
        # get the matched latex expression string
        prefix = matchobj.group(1)
        name = matchobj.group(2)        
        return(prefix + localpath_replacement + name + '"')


# Summarise a post and write the summary to a file. Summary is generated using
# pandoc with <templates>/ssst-summary-entry.html as template. The length of
# the summary is determined by 'summary_length' (-z option). Fixes any links to
# media in the directory containing the post) by replacing them with their full
# path from the root of the destination tree (where the home page is where this
# summary will be included)
# (Note: make sure that this template does NOT generate a standalone HTML
# document; it is included in another page).
# - post: post to summarise
# - fname: name of file to store summary in
def summarize_post(post: Path, fname: str):
    global localpath_replacement
    date = post_metadata[post][0]
    # create a URL for the  post to be summarised
    post_url = item_url(post,Path('index.html'))
    post_path = src_path_of(post)                    
    summary_entry_template = get_template('ssst-summary-entry.html')
    # set the pandoc command string
    command = 'head -n ' + str(summary_length) + ' ' + str(post_path) + \
            ' | pandoc -s -f markdown --template=' + str(summary_entry_template) + \
        ' --metadata entrydate="' + normalize_date(date) + '"' + \
        ' --metadata root="' + str(item_root(Path('index.html'))) + '"' + \
        ' --metadata path="' + post_url + '"' + \
        ' -o '+ str(fname)
    run_shell_command(command)
    # fix any local links in the output (that point to local media in the
    # directory containing the post) by replacing them with their full path
    # from the root of the destination tree (where the home page is where this
    # summary will be included)
    # FIXME: this assumes that summaries are only included in the home page
    localpath_replacement = str(post.parent)
    with open(fname,'r') as f:
        # read the file as one string
        orig_html = f.read()
    processed_html = localpath_re.sub(substitute_localpath, orig_html, count=0)
    # write the result back to the file
    with open(fname,'w') as f:
        f.write(processed_html)

    
# Make the home page, containing a summary of the five most revent posts
# Home page is generated using pandoc, with <templates>/ssst-homepage.html
# as template and <src>/index.md as markdown input
def make_home():
    log(0,'Making home page.')
    # get the 5 most recent posts 
    recent_posts = all_posts[0:5]
    # creata a temorary direcotry to work in; will be deleted when with closes
    with tempfile.TemporaryDirectory() as tmpdir:
        count = 0
        # string keeping track of which temporary summary file to include later
        pandoc_append_str = ''
        for post in recent_posts:
            # create a temporary html file summarizing this entry
            count = count + 1
            summary_name = tmpdir + '/summary.' + str(count) + '.html'
            summarize_post(post,summary_name)
            # and add this to the files to append when making the home page
            pandoc_append_str = pandoc_append_str + ' -A ' + summary_name 
        # Create the home page, including the summarized entries.    
        # Any content in <src_path>/index.md will be prepended before the
        # summarised posts; in particular its metadata determine the title
        # and subtitle
        summary_template = get_template('ssst-homepage.html')
        src = src_path_of('index.md')
        if not src.exists():
          error('Home page input %s does not exist.' % src)
        dest = dest_path_of('index.html')
        dest_tmp = dest.with_suffix('.tmp')
        # set the pandoc command string
        command = 'pandoc -s -f markdown -t html --template=' + \
            str(summary_template) + \
            pandoc_append_str + \
            ' --metadata root=' + str(item_root(Path('index.html'))) + \
            ' -o '+ str(dest_tmp) + ' ' + str(src)
        run_shell_command(command)
        # replace file containing the home page only if its contents changed
        replace_when_changed(dest,dest_tmp)    


# Summarise a post and write the summary to a file. Summary is generated using
# pandoc with <templates>/ssst-rss-entry.html as template. The length of
# the summary is determined by 'summary_length' (-z option). 
# (Note: make sure that this template does NOT generate a standalone HTML
# document; it is included in another page).
# - post: post to summarise
# - fname: name of file to store summary in
def rss_feed_for_post(post: Path, fname: str):
    date = post_metadata[post][0]
    # RSS expects dates to be formatted like: Wed, 27 Nov 2013 15:17:32 GMT
    # (Note that post_metadata[post][0] contains only a date, and not a time) 
    datestr = date.strftime('%a, %d %b %Y 00:00:00 +0000')
    # create a URL for the  post to be summarised
    post_url = item_url(post,Path('index.html'))
    post_path = src_path_of(post)                    
    rss_entry_template = get_template('ssst-rss-entry.html')
    # set the pandoc command string
    command = 'head -n ' + str(summary_length) + ' ' + str(post_path) + \
            ' | pandoc -s -f markdown --template=' + str(rss_entry_template) + \
        ' --metadata entrydate="' + datestr + '"' + \
        ' --metadata root="' + str(item_root(Path('index.html'))) + '"' + \
        ' --metadata path="' + post_url + '"' + \
        ' -o '+ str(fname)
    run_shell_command(command)

        
# Make the RSS feed
def make_rss_feed():
    log(0,'Making RSS feed.')
    # get the 10 most recent posts 
    recent_posts = all_posts[0:10]
    # creata a temorary directory to work in; will be deleted when with closes
    with tempfile.TemporaryDirectory() as tmpdir:
        # first create summaries for each of the recent posts
        count = 0
        # string keeping track of which temporary rss feed file to include later
        pandoc_append_str = ''
        for post in recent_posts:
            # create a temporary html file summarizing this entry for rss
            count = count + 1
            rss_feed_name = tmpdir + '/rss_feed.' + str(count) + '.html'
            # and add this to the files to append when making the home page
            pandoc_append_str = pandoc_append_str + ' -A ' + rss_feed_name
            # create the html summary for the rss feed
            rss_feed_for_post(post,rss_feed_name)
        # Create the RSS feed, including the summarized entries.    
        # The metadata in <src_path>/index.md will determine the title
        # and subtitle; use ssst-rss.html as template
        rss_template = get_template('ssst-rss.html')
        src = src_path_of('index.md')
        if not src.exists():
          error('Home page input %s does not exist.' % src)
        dest = dest_path_of('feed/index.html')
        # create feed folder if it doesnt exist yet
        os.makedirs(dest.parent, exist_ok=True)
        # get the current date and time and set the timezone
        now = datetime.datetime.today().astimezone(ZoneInfo('UTC'))
        # RSS expects dates to be formatted like: Wed, 27 Nov 2013 15:17:32 GMT
        datestr = now.strftime('%a, %d %b %Y %H:%M:%S %z')
        # set the pandoc command string to creat the rss feed html, merging
        # in the entry summaries generated earlier
        command = 'pandoc -s -f markdown -t html --template=' + \
            str(rss_template) + \
            pandoc_append_str + \
            ' --metadata root=' + str(item_root(Path('index.html'))) + \
            ' --metadata date="' + datestr + '"' + \
            ' -o '+ str(dest) + ' ' + str(src)
        run_shell_command(command)
        # rss feed always changes (because the date inside always changes)
        dest_root_changes.append(dest.relative_to(dest_root))


# ---
# REPORT CHANGES / CREATE UPLOAD SCRIPT
# ---

# Report any changed destination files in the log
def report_changes():
    if not dest_root_changes:
        log(0,'No files changed in destination.')
    else:
        log(0,'Files changed in destination:')
        for p in dest_root_changes:
            log(1,str(p))

# Create upload script that uploads all changed files in the destination
def create_uploadscript ():
    # create uploadscritp (if requested)
    if (uploadscript != ''):
        if (uploadcommand == ''):
            error('No upload command string specified')
        log(0,'Creating upload script')
        with open(uploadscript,'w') as f:
            for p in dest_root_changes:
                # write the command to upload changed file, replacing the
                # modifiers {path} and {file} in uploadcommand with the
                # parent folder and the filename of the file
                f.write(uploadcommand.format(
                    path = p.parent,
                    file = p.name
                ))
                f.write('\n')


# ---
# MAIN
# ---

# Set up arguments parsing
parser = argparse.ArgumentParser(description='SSST. A simple static site tool to maintain websites based on markdown and pandoc.')
parser.add_argument('-s', '--source', default=src_root,
                    help='Path to the root of the source files tree')
parser.add_argument('-d', '--destination', default=dest_root,
                    help='Path to the root of the destination files tree')
parser.add_argument('-r', '--root', default=hosting_root,
                    help='Root where the site is hosted. If omitted, the root is found using relative addressing, making the site relocatable')
parser.add_argument('-t', '--templates', default=templates,
                    help='Path to directory with templates')
parser.add_argument('-v', '--verbosity', default=loglevel,
                    help='Verbosity (-1, the default, is silent)')
parser.add_argument('-l', '--logfile', default='ssst.log',
                    help='Name of file to store all log messages in')
parser.add_argument('-u', '--uploadscript', default='',
                    help='Name of script file to store upload commands in')
parser.add_argument('-c', '--uploadcommand', default='',
                    help='Upload command string (with modifiers)')
parser.add_argument('-f', '--force', default = False, action='store_true',
                    help='(Re)make everything')
parser.add_argument('-g', '--gladtex', default = False, action='store_true',
                    help='Use pandoc --gladtex to process LaTeX equations.')
parser.add_argument('-k', '--keepsimple', default = False, action='store_true',
                    help='Do not process simple LaTex equations')
parser.add_argument('-p', '--pedantic', default = False, action='store_true',
                    help='Abort after warning')
parser.add_argument('-z', '--summarylength', default=summary_length,
                    help='Number of lines in post (including YAML header) to include in a summary')
parser.add_argument('-o', '--overwrite', default = False, action='store_true',
                    help='Overwrite files even when contents unchanged')
args = parser.parse_args()

# Apply arguments and check validity
src_root = Path(args.source)
if not src_root.exists():
    error('Source tree %s does not exist.' % src_root)
dest_root = Path(args.destination)
if not dest_root.exists():
    error('Path to destination tree %s does not exist.' % dest_root)
hosting_root = args.root
templates = Path(args.templates)
if not templates.exists():
    error('Templates directory %s does not exist.' % templates)
loglevel = int(args.verbosity)
force = args.force
overwrite = args.overwrite
use_gladtex = args.gladtex
pedantic = args.pedantic
keep_simple_eqs = args.keepsimple
logname = args.logfile
uploadscript = args.uploadscript
uploadcommand = args.uploadcommand
summary_length = int(args.summarylength)

# open logfile
if logname != '':
    logfile = open(logname,'w')

# first collect all metadate (like tags and categories, but also title and date
# for all blog entries (posts and pages) in the source tree. This initializes
# - posts_with_cat (dictionary of all categories, listing all posts contained)
# - posts_with_tag (dictionary of all tags, listing all posts tagged as scuh)
# - post_metadata (dictionary of all posts, containing metadata
#   as tuples (date,title,template)
# - all_posts (essentially a copy of post_metadata): list of all relative
#   pathnames for posts, sorted by date (most recent first)
process_posts()

# dump the category and tag files in ./category/<categorystring>/index.sst and
# ./tag/<tagstring>/index.sst, and create the overviews ./categories.sst
# and ./tags.sst
#
# make sure not to touch files whose contents do not change (so they do not
# show up as a worklist() item and will be skipped by make())
dump_cats_and_tags()

# dump all archive files ./yyyy/mm/index.sst and ./archives.sst
#
# make sure not to touch files whose contents do not change (so they do not
# show up as a worklist() item and will be skipped by make())
dump_archives()

# make all html files whose sources have changed
# (this includes the .sst files just generated)
for item in worklist():
    make(item)

# create the home page containing a summary of the most recent posts
make_home()

# create the rss feed
make_rss_feed()

# report all changed files
report_changes()

# create the upload script to upload all chaned files
create_uploadscript()
        
# close the logfile if necessary
if logfile != None:
    logfile.close()


