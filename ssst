#!/usr/local/bin/python3
#
# SSST
#
# A simple static site tool to maintain websites based on markdown and pandoc
#
# Author: Jaap-henk Hoepman (jhh@xs4all.nl)

import argparse, sys, os, re, tempfile, subprocess, shutil, datetime, yaml, filecmp, stat
from pathlib import Path
from io import StringIO
from zoneinfo import ZoneInfo

# ---
# DelayedTextfile
#
# Create a textfile, whose contents is only actually written to external
# storage when closing the file and the new contents are different from
# the old contents (or when the file does not exist yet)
# ---

class DelayedTextfile(StringIO):

    # create a Delayedtextfile for external file fname,
    # to be written on file close
    def __init__(self,fname):
        super().__init__()
        self.path = Path(fname)

    # return true if the file was actually written, false otherwise    
    def close(self):
        newcontents = self.getvalue()
        if self.path.exists():
            with open(self.path,'r') as f:
                oldcontents = f.read()
                if oldcontents == newcontents:
                    log(5,'File not changed ' + str(self.path))
                    return False
        # create parent directory; necessary if eg date in post
        # doesn't correspond with path, so archive file will be
        # written in an not-yet-existing path
        os.makedirs(self.path.parent, exist_ok=True)
        with open(self.path,'w') as f:
            f.write(newcontents)
            log(5,'File changed ' + str(self.path))
            return True

    # Allow with DelayedTextfile(name) as f: constructions
    # where file is automatically closed when with statement ends
    def __exit__(self, type, value, tb):
        self.close()

# ---
# CONSTANTS / PARAMETERS
# ---

# All items processed by SSST are *relative* paths to posts or pages, relative
# to the src_root and dest_root prefixes specified on the commandline
# and 'declared' as global variables here

# Path to the root of the source files tree
src_root = Path('./src')

# Path to the root of the destination files tree
dest_root = Path('./public')

# Root where the site is hosted
# (relative adressing, making the whole site relocatable, is the default)
hosting_root = ''

# Path to directory with templates
templates = Path('./templates')

# Verbosity
loglevel = -1

# Force remaking all files
force = False 

# Overwrite files even when contents are unchanged
overwrite = False 

# Use pandoc --gladtex to process LaTeX equations
use_gladtex = False

# Abort after warning
pedantic = False 

# Do not process simple LaTeX equations (pass them through in italics)
keep_simple_eqs = False

# Lenght of summaries (including YAML header)
summary_length = 20

# logfile name
logfile = None 

# upload script file name
uploadscript = None 

# upload command for upload script, can containt the following modifiers
# {path} : replaced with the destination path of the changed item (without the
#    destination prefix specified by -d
# {src} : full path name of the changed file that needs to be uploaded
uploadcommand = ''

# delete command for upload script, can containt the following modifiers
# {path} : replaced with the destination path of the changed item (without the
#    destination prefix specified by -d
# {src} : full path name of the changed file that needs to be uploaded
deletecommand = ''

# ---
# OTHER GLOBAL VARIABLES
# ---

# Dictionary of all tags, listing for each tag the relative pathnames for the posts with those tags
# (filled by process_posts -> process_post_metadata) 
posts_with_tag = {}

# Dictionary of all categories, listing for each category the relative pathnames for the posts with those categories
# (filled by process_posts -> process_post_metadata) 
posts_with_cat = {}

# Dictionary of all posts (indexed using their relative pathnames), containing
# the metadata as tuples (date,title,template)
# (filled by process_posts -> process_post_metadata) 
post_metadata = {}

# List of all relative pathnames for posts, sorted by date (most recent first)
# (with the help of process_posts)
all_posts = []

# ---
# ERRORS / LOGGING
# ---

# Outpur the error message, and log it when required. Then exit
# - msg: the error message
def error(msg: str):
    m = 'ssst: error: %s\n' % str(msg)
    sys.stderr.write(m)
    if logfile != None:
        logfile.write(m)
    sys.exit (1)

# Output the warning message, and write it to the log file if it was specified.
# Exit when pedantic processing was specified.
# - msg: the warning message
def warning(msg: str):
    m = 'ssst: warning: %s\n' % str(msg)
    sys.stderr.write(m)
    if logfile != None:
        logfile.write(m)
    if pedantic:
        sys.exit (1)

# Output a log message when its level is lower than the specifief verbosity
# on the command line. Always write the message to the log file if it was
# specified 
# - level: the log-level for this message
# - msg: the log message
def log (level: int, msg: str):
    if level == 0:
        m = '# '
    else:
        m = '-' * level + ' '
    m = m + str(msg) + '\n'
    if (level <= loglevel):
        sys.stdout.write(m)
    if logfile != None:
        logfile.write(m)


# ---
# SHELL
# ---

# Run a shell command. Abort and print the error message when it fails
# - command
def run_shell_command(command: str):
    log(4,'Running shell command ' + command)
    result = subprocess.run(command,text=True,shell=True,capture_output=True)
    if result.returncode != 0:
        error(result.stderr)

# ---
# PANDOC PROCESSING
# ---

# Process a markdown source file to create the corresponding html destination
#
# - src : item to process (relative to src)
# - dest_path : full path of destination file
# - container : item in which processed file is included (src if not)
#   (used to determine root and  path metadata)
# - summarize: if true, include only summary_length lines of the input
# - template: name of template to use
# - appends: other html files pandoc should append (-A)
# - metadata: dictionary of metadata items to include in the pandoc commandline
#  (--metadata); only items with a value != '' are included
# - datestr: string representing date, to be included as metadata as well
def pandoc ( src: Path, dest_path: str, container: Path, summarize: bool \
           , template: str, appends: list, metadata, datestr: str):
    src_path = src_path_of(src)
    if not src_path.exists():
        error('Page input %s does not exist.' % src)
    tpath = get_template(template)
    if use_gladtex:
        gladtex_str = ' --gladtex '
    else:
        gladtex_str = ''
    # create append string
    append_str = ''
    for a in appends:
        append_str = append_str + ' -A ' + str(a)
    # create metadata string; include date metadata
    metadata['date'] = datestr
    metadata['root'] = str(item_root(container))
    metadata['path'] = str(item_url(src,container))    
    metadata_str = ''
    for m in metadata:
        if str(metadata[m]) != '':
            metadata_str = metadata_str + \
                ' --metadata ' + str(m) + '="' + str(metadata[m]) + '"'
    if summarize:
        command = 'head -n ' + str(summary_length) + ' ' + str(src_path) \
            + ' | pandoc -s -f markdown -t html --template=' + str(tpath) \
            + gladtex_str + append_str + metadata_str \
            + ' -o '+ str(dest_path) 
    else:
        command = 'pandoc -s -f markdown -t html --template=' + str(tpath) \
            + gladtex_str + append_str + metadata_str \
            + ' -o '+ str(dest_path) + ' ' + str(src_path)
    run_shell_command(command)
    

# ---
# YAML
# ---

# Return the dictionary of entries found in the YAML block at the start of
# a file
# - src: full path to the file
def get_yaml_dict(src: Path):
    try:
        with open(src,'r') as f:
            yamls = yaml.load_all(f,Loader=yaml.SafeLoader)
            for y in yamls:
                return y
    except:
        warning('Invalid YAML block in ' + str(src))
        return {}

# ---
# FOLDER FUNCTIONS
# ---

# Return the full source folder containing item
# - item: relative path to the item
def src_folder_of(item: Path) -> Path:
    return src_root / item.parent # full source folder of item

# Return the full source folder pathname pointing to item
# - item: relative path to the item
def src_path_of(item: Path) -> Path:
    return src_root / item

# return full file path for a possible source (with suffix specified) for
# the destination specified
# - dest: destination to find the source for
# - suffix: source suffix
def possible_src_of(dest: Path, suffix: str) -> Path:
    item = dest.relative_to(dest_root)
    src = src_root / item
    return(src.with_suffix(suffix))

# Return the relative path (wrt src_root) for the full path leading to an item
# -item: full path in source tree to item
def item_relpath(item: Path) -> Path:
    return item.relative_to(src_root)

# Return the full destination folder to store the result for item
# - item: relative path to the item
def dest_folder_of(item: Path) -> Path:
    return dest_root / item.parent # full source folder of item

# Return the full destination path for the html file to be generated
# for the item
# - item: relative path to the item
def dest_path_of (item: Path) -> Path:
    dest = dest_root / item
    return dest.with_suffix('.html')

# Return an iterator that iterates over all files in the tree rooted at
# the specified root with the specified suffix
# - root: root of the tree to scan
# - suffix: suffix of files to return, including "."
def all_files_with_suffix(root: Path, suffix: str):
    return root.glob('**/*' + suffix)

# Return an iterator that iterates over all files in the tree rooted at
# src_root with the specified suffix
# - suffix: suffix of files to return, including "."
def src_files_with_suffix(suffix: str):
    return all_files_with_suffix(src_root,suffix)


# Return an iterator that iterates over all files in the tree rooted at
# dest_root with the specified suffix
# - suffix: suffix of files to return, including "."
def dest_files_with_suffix(suffix: str):
    return all_files_with_suffix(dest_root,suffix)

# Determine list of Paths containing the comments (empty if none) for the item
# I.e. all files matching <src_root> / <parent_folder> / *.mdc
# - item: relative path to the item
def commentfiles_for (item: Path):
    return (src_root / item.parent).glob('*.mdc')

# Suffix of media files that will automatically be copied from the source
# folder to the destination folder
media_types = ['jpg','gif','pdf','png']

# Determine list of Paths containing the mediafiles (empty if none) for the item
# I.e. all files matching <src_root> / <parent_folder> / <mediatypes>
# - item: relative path to the item
def mediafiles_for (item: Path):
    res = []
    for tp in media_types:
        res = res + list((src_root / item.parent).glob('*.' + tp))
    return res

# Return the relative pathname for the intermediate file containing all
# posts belonging to a category
# - cat: the category
def cat_pathname_of ( cat: str ) -> Path:
    return Path('category/' + cat + '/index.sst')

# Return all full pathnames for the all intermediate category information files
# that are stored on disk
def existing_cat_pathnames ():
    return (src_root / Path('category')).glob('*/index.sst')


# Return the relative pathname for the intermediate file containing all
# posts belonging to a tag
# - tag: the category
def tag_pathname_of ( tag: str ) -> Path:
    return Path('tag/' + tag.lower() + '/index.sst')


# Return all full pathnames for the all intermediate tag information files
# that are stored on disk
def existing_tag_pathnames ():
    return (src_root / Path('tag')).glob('*/index.sst')

# Return path pointing to the root for the current item on the hosting site.
# This is either a relative path back to the root from the path containing the
# item, or simply hosting_root if set to non-empty on the command line 
# - item: relative path to item
def item_root (item: Path) -> str: 
    if hosting_root != '':
        return hosting_root
    else:
        item = str(item)
        log(5,'Computing root for ' + item)
        res = ''
        for c in item:
            if c == '/':
                res = res + '../'
        # remove trailing backslash (does nothing when res == '')
        res = res[:-1]
        if res == '':
            res = '.'
        log(5,'Computed root ' + res)    
        return res

# Determine the link pointing to the item on the hosting site
# relative to context, i.e.
# the generated link works if is included in the page with path context
# - item: relative path to item
# - context: relative path to page that will contain the link to item 
def item_url (item: Path, context: Path) -> str:
    if ((item == None) or (item == '')):
        return('')
    else:
        item = item.with_suffix('.html')
        return str(item_root(context) / item)

# Return the full path for a template, throw error if it does not exist
# - name: template file name to be found in the template directory  
def get_template(name: Path):
    template = templates / name        
    if not template.exists():
        error('Template %s not found' % template)
    return template

# ---
# OTHER FUNCTIONS
# ---

# Replace the contents of a file with new contents, but only change the file
# and its metadata when the new contents are different from the current
# contents.
# - current: file whose contents must be updated 
# - update: file containing new content
# return: whether file changed
def replace_when_changed (current: Path, update: Path) -> bool:
    if (not current.exists()) \
       or overwrite \
       or (not filecmp.cmp(current,update,shallow=False)): 
        log(5,'File changed ' + str(current))
        update.rename(current)
        return True
    else:
        log(5,'File not changed ' + str(current))
        update.unlink() # remove file
        return False

# Parse a date if it is a string, return unchanced otherwise.
# (Only parses wordpress generated dates like 'Sun, 19 Apr 2020 08:42:00 +0000'
# that the YAML parser fails to parse)
# - s: string to parse if it isn't a date
def parse_date (s) -> datetime.date:
    if type(s) is str:
        time = datetime.datetime.strptime(s,'%a, %d %b %Y %H:%M:%S %z')
        return time.date()
    else:
        return s

# Date format string constants
rss_date_format_str = '%a, %d %b %Y %H:%M:%S +0000'
page_date_format_str = '%B %-d, %Y'
    
# Minimum amount of milliseconds difference between destination and source 
# modification time before an item will be considered older (and hence a
# workitem)

grace_period = 10

# Check whether dest is older than src
# - dest
# - src
def is_older(dest,src: Path):
    dtime = os.stat(dest)[stat.ST_MTIME]
    stime = os.stat(src)[stat.ST_MTIME]
    return dtime < (stime - grace_period)

# ---
# PROCESS POSTS
# ---

# List of normalised tags seen, to detect different capitalisations of
# tags (like iOS and Ios) that are considered different but would
# be written to the same external tag file if the filesystem
# is case-insensitive; one overwriting the other!!!
normalised_tags = []

# Process post metadata and store it in post_metadata[].
# Determine the categories and the tags assiociated with a post 
# and add its  pathname to the necessary categories and tag dictionaries
# - post : full pathname to the post to process
def process_post_metadata(post: Path):
    global posts_with_tag
    global posts_with_cat
    global post_metadata
    log(2,'Processing categories and tags for ' + str(post))
    yaml = get_yaml_dict(post)
    post_relpath = item_relpath(post)
    if 'categories' in yaml:
        cats = yaml['categories']
        log(3,'Categories: ' + str(cats))
        # careful: categories may be a single string
        if type(cats) is str:
            cats = [ cats ]
        for cat in cats:
            # make sure categoreis are stored in all lowercase
            # (e.g. ios and iOS would otherwise be considered different 
            # but written to the same external file if the filesystem
            # is case-insensitive; one overwriting the other!!!
            cat = cat.lower()
            if cat not in posts_with_cat:
                posts_with_cat[cat] = []
            posts_with_cat[cat].append(post_relpath)
    if 'keywords' in yaml:
        tags = yaml['keywords']
        log(3,'Tags: ' + str(tags))
        # careful: tags may be a single string
        if type(tags) is str:
            tags = [ tags ]
        for tag in tags:
            # captilize without changing caps of other letters
            tag = tag[:1].capitalize() + tag[1:]
            if tag not in posts_with_tag:
                # check if tag is not clashing with existing tag
                normalised_tag = tag.lower()
                if normalised_tag in normalised_tags:
                    warning('Different capitalisation of ' + normalised_tag + \
                            ' encountered in ' + str(post) + ".")
                else:
                    normalised_tags.append(normalised_tag)     
                posts_with_tag[tag] = []
            posts_with_tag[tag].append(post_relpath)
    if 'title' in yaml:
        title = yaml['title']
    else:
        title = ''
        warning('Title expected but not found in ' + str(post) + \
                '. Using empty title.')
    if 'date' in yaml: 
        tmp = yaml['date']
        date = parse_date(tmp)
        dateprefix = date.strftime('%Y/%m/%d')
        poststr = str(item_relpath(post))
        if not poststr.startswith(dateprefix):
            warning('Date ' + dateprefix + ' in ' + str(post) + \
                    ' does not correspond with its path.')
    else:
        error('Date expected but not found in ' + str(post))
    if 'template' in yaml:
        template = yaml['template']
    else:
        template = ''
    post_metadata[post_relpath] = (date,title,template)

# Regular expression matching a path for a post (i.e. one that starts with a
# four digit year)
# FIXME: this is incomplete
post_re = re.compile('[0-9][0-9][0-9][0-9].*')
    
# Determine whether the item is a post (or a page)
# - item: relative path to item to determine type of
def is_post(item: Path) -> bool:
    return (post_re.match(str(item)) != None) and (item.suffix == '.md')


# Collect all metadata about all posts (like tags and categories, but also
# title and date for all blog entries) in the source tree. This initializes
# the following global variables:
# posts_with_cat, posts_with_tag, post_metadata and all_posts
def process_posts ():
    log(0,'Processing posts.')
    # Find all posts (they have suffix .md) and process their metadata
    for item in src_files_with_suffix('.md'):
        if is_post(item_relpath(item)):
            process_post_metadata(item)
    # sort all posts by date and store the list of relative paths in all_posts
    for post in post_metadata:
        all_posts.append(post)
    # sort in decending order
    all_posts.sort(reverse=True,key=lambda entry: post_metadata[entry][0])
    

# ---
# LaTeX EQUATIONS
# ---

# number of matches so far
eq_match_count = 0
# folder containing the currently processed file
basefolder = ''
# dictonary of processed equations, containing for each equation the
# name of the SVG file containing its image;
# reset for every file
equations = {}

# list of svg file names that changed
changed_svgs = []

# Return the filename of the file to store the SVG for the i-th equation in
def get_svg_file_name(i):
    return 'index.' + str(i) + '.svg'

# Return the full pathname of the file to store the SVG for the i-th equation in
def get_svg_path_name(i):
    return basefolder / get_svg_file_name(i)

# Turn a latex expression into an SVG graphic and write it to file.
# The filename to use is determined by the global variable basefolder
# (derived from the name of the file containing the equations) and
# number of processed equations for that file
# - latex_expr: latex expression (without surrounding $$)
# - returns: the name of the SVG file in which the graphic is stored
#         (relative to basename)
def process_eq(latex_expr):
    global eq_match_count
    global changed_svgs
    eq_match_count = eq_match_count + 1
    # create the SVG output name
    svg_file_name = get_svg_file_name(eq_match_count) 
    svg_path_name = get_svg_path_name(eq_match_count) 
    svg_tmp = svg_path_name.with_suffix('.tmp')
    log(4,'Storing equation in ' + str(svg_path_name))
    # creata a temorary direcotry to work in; will be deleted when with closes
    with tempfile.TemporaryDirectory() as tmpdir:
        tmp_tex = tmpdir + '/htsv.tex'    
        tmp_pdf = tmpdir + '/htsv.pdf'
        # read the latex template and store it as a string in t
        eq_template = get_template('ssst-eq.tex')
        with open(eq_template,'r') as f:
            latex_template = f.read()
        # insert latex_expr at $$ in the template
        s = latex_template.split('$$')
        latex_document = s[0] + '$'+ latex_expr + '$' + s[1]
        # write the result to a tex file in the temporary directory
        with open(tmp_tex,'w') as w:
            w.write(latex_document)
        # create the pdf (in the temporary directory) from the latexfile
        run_shell_command('pdflatex -output-directory ' + \
                             tmpdir + ' ' + tmp_tex)
        # create the SVG from the pdf
        run_shell_command('pdf2svg ' + tmp_pdf + ' ' + str(svg_tmp))
        # replace destination only when real changes occured
        if replace_when_changed(svg_path_name,svg_tmp):
            changed_svgs.append(svg_path_name.relative_to(dest_root))
    return svg_file_name

# Match &lt;
lessthan_re = re.compile(r'(&lt;)')

# Match &gt;
greaterthan_re = re.compile(r'(&gt;)')

# Replace any occurance of '&lt;' on the input with '<'
# (because the --gladtex option of pandoc turns < into &lt;)
def sanitize_eq (expr):
    ret = lessthan_re.sub('<',expr,count=0)
    ret = greaterthan_re.sub('>',ret,count=0)    
    log(5,'Sanitized '+ expr + ' to ' + ret)
    return ret

# Match a simple expression, that can be passed without processing
simple_expr_re =  re.compile(r'([A-Za-z0-9])+')

# Process currently matched latex equation to create an SVG for it
# return the replacement text for the currently matched latex equation
# (ie an <img> tag pointing the SVG file generated for it)
# If the exact same equation was already made, immediately return the
# <img> tag pointing to the SVG file previously made.
# - matchobj: object found to match the equation regular expression 
def substitute_eq (matchobj):
    if matchobj:
        # get the matched latex expression string
        match = matchobj.group(1)
        # sanitize expressions
        latex_expr = sanitize_eq(match)
        log(4,'Processing equation ' + latex_expr)
        # do not process simple LaTex equations, when so instructed
        if keep_simple_eqs and simple_expr_re.fullmatch(latex_expr):
            return '<em>' + latex_expr + '</em>'
        if latex_expr in equations:
            log(5,'Already made!')
            svg_file_name = equations[latex_expr]
        else:
            svg_file_name = process_eq(latex_expr)
            equations[latex_expr] = svg_file_name
        # return the <img> tag that points to the generated SVG
        return '<img class="latexeq"' \
            + ' src="' + svg_file_name + '"' \
            + ' alt="' + latex_expr + '">'
    
# Match <eq ...>LATEXEXPRESSION</eq> generated by pandoc -gladtex
# (use .*? for non-greedy matching, to ensure the first </eq> found
# will close the regulear expression)
gladtex_eq_re = re.compile(r'<eq[^>]*>(.*?)</eq>',re.DOTALL)

# Match <span class="math inline">$LATEXEXPRESSION$</span> generated by
# pandoc without any math processing option.
# (use .*? for non-greedy matching, to ensure the first $</span> found
# will close the regulear expression)
plain_eq_re = re.compile(r'<span class="math inline">\$(.*?)\$</span>',re.DOTALL)

# Process any equations in the item (determined using the regular
# expression eq_re)
# - fname: full path of item to be processed (note: the parent is the folder
#      in which any equation images are stored)
# return: list of changed cvg files
def process_equations (fname) -> list:
    global eq_match_count
    global basefolder
    global equations
    global changed_svgs
    log(2,'Processing equations.')
    eq_match_count = 0
    equations = {}
    changed_svgs = []
    basefolder = fname.parent
    # open the file for reading
    with open(fname,'r') as f:
        # read the file as one string
        orig_html = f.read()
    # process and substitute each occurence of a latex equation
    if use_gladtex:
        processed_html = gladtex_eq_re.sub(substitute_eq, orig_html, count=0)
    else:
        processed_html = plain_eq_re.sub(substitute_eq, orig_html, count=0)
    # write the result back to the file
    with open(fname,'w') as f:
        f.write(processed_html)
    # remove any SVG fles containing equations from previous runs
    # FIXME: PASS DELETED STATUS
    eq_match_count = eq_match_count + 1
    svg_path_name = get_svg_path_name(eq_match_count) 
    while svg_path_name.exists():
        svg_path_name.unlink() # remove file
        eq_match_count = eq_match_count + 1
        svg_path_name = get_svg_path_name(eq_match_count) 
    return changed_svgs

# ---
# MAKE ITEM
# ---

# return the path to next post (more recent) following item; '' if none
# (this uses the fact that all_posts is sorted by date in descending order)
def next_post(item):
    if item in all_posts:
        pos = all_posts.index(item)
        if pos-1 >= 0:
            return all_posts[pos-1]
    return ''

# return the path to previous post (less recent) following item; '' if none
# (this uses the fact that all_posts is sorted by date in descending order)
def prev_post(item):
    if item in all_posts:
        pos = all_posts.index(item)
        if pos+1 < len(all_posts):
            return all_posts[pos+1]
    return ''

# Make the html item from the md or sst source specified by item, merging
# in any comments, and using item_template as the template
# return: list of changed svg files and the destination of the item itself
def make_item_merge_comments(item,commentcount,comments,template):
    dest = dest_path_of(item)
    # create destination folder if it doesnt exist yet
    os.makedirs(dest_folder_of(item), exist_ok=True)
    # blog-post path for reply links: strip the source prefix in front
    # and the index.md at the end; add comment.
    reply_path = str(item.parent) + '/comment.'
    # build the metadata dictionary
    metadata = { 'reply' : reply_path
               , 'next' : item_url(next_post(item),item)
               , 'prev' : item_url(prev_post(item),item) }
    if commentcount > 0:
       metadata['commentcount'] = str(commentcount)
    if item in post_metadata:
       date = post_metadata[item][0]
       datestr = date.strftime(page_date_format_str)
    else:
       datestr = ''
    pandoc(item, dest, item, False, template, [ comments ], metadata, datestr)
    # process any equations in the output
    # FICME: check return rewlative  paths
    changed_svgs = process_equations(dest)
    changed_svgs.append(dest.relative_to(dest_root))
    return(changed_svgs)
    
# ---
# MAKE COMMENTS
# ---

# Assume all comment files have been converted to html and are stored in
# the temporary directory. 
# Recursively fold all coments at the current level (specified by path in
# the temporary directory)
# and return them as a string; try: <path>.1.html (and fold its children)
# then  <path>.2
def fold_comments(path):
    log(4,'Folding ' + str(path)) 
    folded = ''
    i = 1
    while True:
        prefix = path + '.' + str(i)
        filename = prefix + '.html'
        if not os.path.exists(filename):
            break
        with open(filename,'r') as f:
            folded = folded \
                + '<div class="comment">\n'\
                + f.read() \
                + fold_comments(prefix) \
                + '</div>\n'
        i = i + 1
    return folded
    
# Process the comments associated with the item. 
# - first convert individual comments to html in tmpdir
# - then fold them into one html file (keeping the hierarchical order)
# Stores the result in tmpdir/comments.html for further processing
# by make (this file is empty if there are no comments)
def make_comments(item,tmpdir):
    log(2,'Processing comments')
    commentfiles = commentfiles_for(item)
    commentcount = 0
    # first convert all .mdc files in src_root to .html files in tmpdir
    for c in commentfiles:
        # strip of src/ prefix
        rel_c = item_relpath(c)
        commentcount += 1
        # filename to store html converted comment in (in tmpdir)
        outf = tmpdir + '/' + str(c.name)[:-3] + 'html'
        # blog-post path and comment depth indicator for reply links
        # strip mdc from the back
        reply_path = str(rel_c)[:-3]
        log(3,'Making comment HTML ' + outf)
        metadata = { 'reply' : reply_path }
        pandoc(rel_c, outf, rel_c, False, 'ssst-comment.html', [], metadata, '')
    # now recursively fold in comments created in tmpdir
    comments = fold_comments(tmpdir + '/comment')
    log(5,comments)
    # and store the comments in comments.html in the tmpdir
    with open(tmpdir + '/comments.html','w') as f:
        f.write(comments)
    return commentcount

# ---
# GENERATE AUXILIARY FILES
# ---

# Write a link (entry_url) describing an entry (using its title and date)
# to the output stream outf
def dump_entry(entry,outname,outf):
    (date,title,template) = post_metadata[entry]
    datestr = date.strftime(page_date_format_str)
    outf.write('- [' + title + '](' + item_url(entry,outname) + \
               ') (' + datestr + ')\n')    

# Create a markdown document into out_name (relative path)
# (with title as title and label as subtitle)
# containing a list of links to all posts in filelist
# (assumed to be sorted by date)
def dump_entries(out_name,type,label,filelist):
    # sort the list of pathnames by date, most recent first
    # (FIXME: using the fact that the path encodes the file date)
    filelist.sort(reverse=True)
    os.makedirs(src_folder_of(out_name),exist_ok=True)
    log(3,'Dumping to ' + str(out_name))
    out_pathname = src_path_of(out_name)
    with DelayedTextfile(out_pathname) as outf:
        outf.write('---\n')
        outf.write("title: '" + label + "'\n")
        outf.write("subtitle: '" + type + "'\n")        
        outf.write('---\n')        
        for entry in filelist:
            dump_entry(entry,out_name,outf)


# Generate all tag files in ./tag directories in the src_root (SOURCE!) tree;
# These are markdown files (with suffix .sst) that will subsequently be
# converted to html through the make routine
#
# Files whose contents would not change will not be touched; this way
# make will ignore them as the timestamp remains unchanged
#
# return: the list of all generated files (including unchanged ones) 
def generate_tag_files () -> list:
    log(0,'Generating tag files.')
    tags_name = 'tags.sst'
    tags_pathname = src_path_of(tags_name)
    # keep track of generated files
    generated = [tags_pathname]
    # DelayedTextfile will be written to disk when with closes
    # (and contents really chagned)
    with DelayedTextfile(tags_pathname) as tagsf:        
        tagsf.write('---\n')
        tagsf.write("title: 'Tags'\n")
        tagsf.write('---\n\n')
        # initialise firstchar so that the very first time a header is always
        # written to output
        firstchar = ''
        for tag in sorted(list(posts_with_tag), key=str.lower):
            # output a header to the tags file for each alphabetical change
            if firstchar != tag[0]:
                firstchar = tag[0]
                tagsf.write('\n\n## ' + firstchar.upper() + '\n\n') 
            tag_pathname = tag_pathname_of(tag)
            tagsf.write( '- [' + tag + '](' \
                       + item_url(tag_pathname,tags_name) + ')\n')
            dump_entries(tag_pathname,'Tag',tag,posts_with_tag[tag])
            generated.append(src_path_of(tag_pathname))
    # return the list of all generated files
    return(generated)


# Generate all category files in ./category directories in the
# src_root (SOURCE!) tree;
# These are markdown files (with suffix .sst) that will subsequently be
# converted to html through the make routine
#
# Files whose contents would not change will not be touched; this way
# make will ignore them as the timestamp remains unchanged
#
# return: the list of all generated files (including unchanged ones) 
def generate_category_files () -> list:
    log(0,'Generating category files.')
    categories_name = 'categories.sst'
    categories_pathname = src_path_of(categories_name)
    # keep track of generated files
    generated = [categories_pathname]
    # DelayedTextfile will be written to disk when with closes
    # (and contents really chagned)
    with DelayedTextfile(categories_pathname) as catsf:        
        catsf.write('---\n')
        catsf.write("title: 'Categories'\n")
        catsf.write('---\n\n')
        for cat in sorted(list(posts_with_cat)):
            cat_pathname = cat_pathname_of(cat)
            catsf.write( '- [' + cat + '](' \
                       + item_url(cat_pathname,categories_name) + ')\n')
            dump_entries(cat_pathname,'Category',cat,posts_with_cat[cat])
            generated.append( src_path_of(cat_pathname) )
    # return the list of all generated files
    return(generated)

            
            
# return a sort key for an entry such that entries are sorted
# by decreasing year, but increasing month and day
def archive_sort_key(entry):
    date = post_metadata[entry][0]
    return (-date.year,date.month,date.day)

# Create archive files containing all posts for a particular month in 
# ./yyyy/mm/index.sst in the src_root (SOURCE!) tree;
# Also create a list of all known archives in src_root/archives.sst
# These are markdown files that will subsequently be converted to html
# through the make routine.
#
# Files whose contents would not change will not be touched; this way
# make will ignore them as the timestamp remains unchanged
#
# return: the list of generated files (including unchanged ones)  
def generate_archive_files () -> list:
    log(0,'Generating archives.')
    last_year_archived = 0
    # dictionary with DelayedTextFile descriptors, indexed by archivepath
    archive_paths = {}
    # sort the posts by *decreasing* year, but increasing month/day within
    # a year; this ensures that the main archive file lists the most
    # recent year first
    archive_posts = all_posts.copy()
    archive_posts.sort(key=archive_sort_key)
    # create the root archive file (as a DelayedTextfile
    archives_pathname = src_path_of('archives.sst')
    # keep track of generated files
    generated = [archives_pathname]
    log(3,'Writing all archives to ' + str(archives_pathname))
    # DelayedTextfile will be written to disk when with closes
    # (and contents really chagned)
    with DelayedTextfile(archives_pathname) as archivef:        
        archivef.write('---\n')
        archivef.write("title: 'Archives'\n")
#        archivef.write("template: 'ssst-archives.html'\n")
        archivef.write('---\n')
        # process all posts to be archived and put them in the
        # appropriate archive (determined by the month and year of publication)
        # this creates another DelayedTextfile for every new archive
        for post in archive_posts:
            # construct the full and relative path for the archive this post
            # should appear in
            date = post_metadata[post][0]
            archive_year = date.year
            monthyear = date.strftime('%B %Y')
            archive_path = Path(date.strftime('%Y/%m/index.sst'))
            archive_src_path = src_path_of(archive_path)
            generated.append(archive_src_path)
            # if we are processing a new month
            if archive_path not in archive_paths:
                # write a header every time the year changes to the main
                # archive file (this assumes archive_posts is sorted)
                if last_year_archived != archive_year:
                    last_year_archived = archive_year
                    archivef.write('\n## ' + str(archive_year) + '\n')
                # write a link to this new archive to the main archive file
                archivef.write('- [' + monthyear +'](' + \
                               item_url(archive_path,\
                                        Path('archives.sst')) + \
                               ')\n' )
                # create a new delayed file for this monthly archive
                log(3,'Creating new monthly archive ' + str(archive_path))
                outf = DelayedTextfile(archive_src_path)
                archive_paths[archive_path] = outf
                # write the header for the new archive
                outf.write('---\n')
                outf.write("title: 'Archive " + monthyear + "'\n")        
                outf.write('---\n')
            # append the entry to the monthly archive
            outf = archive_paths[archive_path]
            dump_entry(post,archive_path,outf)
    # get the archive paths in the correct order to dump next/prev pointers
    archive_paths_list = list(archive_paths.keys())
    archive_paths_list.sort()
    # write prev/next pointers and close all archive files
    for i,archive_path in enumerate(archive_paths_list):
        outf = archive_paths[archive_path]
        # write separator
        outf.write('\n\n') 
        # find next and previous archive and add links
        if (i > 0):
            prevurl = item_url(archive_paths_list[i-1],archive_path)
            outf.write('[previous](' + prevurl + ') archive') 
        if (i > 0) and (i < len(archive_paths_list)-1):
            outf.write(' / ')
        if (i < len(archive_paths_list)-1):
            nexturl = item_url(archive_paths_list[i+1],archive_path)
            outf.write('[next](' + nexturl + ') archive')
        outf.write('\n\n') 
        outf.close()
    # return the list of all generated files
    return(generated)


# Generate all auxiliary files (tags, catgories, archives)
# return: the list of generated files 
def generate_auxiliary_files () -> list:
    generated = generate_tag_files()
    generated = generated + generate_category_files() 
    generated = generated + generate_archive_files()
    return(generated)
            
# ---    
# DETERMINE WORKLIST
# ---

# Determine whether the item (given src and dest directories in the global
# variables src_root and dest_root) should be remade: i.e. if the destination
# does not exist, or when it is older than the source file or any of its
# associated comments or media files (within a certain grace period)
# - item: path to item
# - returns: whether item is a workitem or not
def is_workitem(item: Path) -> bool:
    # check whether destination exists
    dest = dest_path_of(item)
    log(4,'Destination should be ' + str(dest))
    if not dest.exists():
        return True
    log(4,'Destination exists.')
    # check whether destination is older than source
    src = src_path_of(item)
    if is_older(dest,src):
        log(4,'Destination older than source.')
        return True
    # check whether destination is older than any comment or mediafile
    # in the source
    files = list(commentfiles_for(item)) + mediafiles_for(item)
    for file in files:
        log(4,'Additional file found: ' + str(file))
        if is_older(dest,file):
            log(4,'Destination older than comment or media item in source.')
            return True
    return False


# match all files with suffix in the tree rooted at src_root and return a
# list of all files that need to be (re)made
# - suffix: suffix of files to consider (.md or .sst)
# - returns: a list of paths to items that need to be made
def worklist_for (suffix: str):
    list = []
    # traverse all files with suffix in the subtree rooted at src
    # (This returns a 'list' of Paths (not strings))
    for src in src_files_with_suffix(suffix):
        log(2,'Considering: ' + str(src))
        # strip the src_root
        item = item_relpath(src)
        # add the item when -f given, or when a workitem
        if force or is_workitem(item):
            log(2,'Adding ' + str(src))
            list.append(item)
        else:
            log(2,'Ignoring ' +  str(src))
    return list

# Determine posts, pages and generated pages that need to be (re)made
# - returns: a list of paths to items that need to be made
def worklist ():
    log(0,'Determining files to process...')
    # determine all .md files that need to be made
    list = worklist_for('.md')
    # remove <src_root>/index.md which is (always) made separetely
    # through make_home() called later
    if Path('index.md') in list:
        list.remove(Path('index.md'))
    extended_list = list.copy()
    # insert any "neighbours" of posts in the worklist as well, to ensure
    # that their next and prev links are updated if necessary
    for item in list:
        if is_post(item):
            prev = prev_post(item)
            if (prev != '') and (prev not in extended_list):
                extended_list.append(prev)
            next = next_post(item)
            if (next != '') and (next not in extended_list):
                extended_list.append(next)
    # add all *.sst files that need to be made
    final_list = extended_list + worklist_for('.sst')
    log(0,str(len(final_list)) + ' files to process found.')
    return final_list

# ---
# MAKE
# ---

# Copy all media associated with an item, keeping their metadata.
# - item: path to item in the source tree
# return: mediafiles changed
def copy_media(item: Path) -> list:
    log(2,'Copying mediafiles')
    dest_folder = dest_folder_of(item)
    changed = []
    for f in mediafiles_for(item):
        log(4,'Copying media file ' + str(f) + ' to ' + str(dest_folder))
        changed.append(item_relpath(f))
        shutil.copy2(f,dest_folder) # keep metadata
    return(changed)

# Remove all non html and non .svg files (not diretories) from a folder
# (Keep the html and svg files because that will be remade, and we want to check
# later whether the new version is the same as the old or not)
# (After making equations any remaining old equation files are removed)
# - folder: folder to clean
def clean_folder(folder: Path):
    log(2,'Cleaning destination ' + str(folder))
    for f in folder.glob('*'):
        if (not f.is_dir()) and (f.suffix != '.html') and (f.suffix != '.svg'):
            os.remove(f)

# Return the template to use to process post: default if none specified
# in the YAML header for item
def post_template(post,default):
    if post in post_metadata:
        template = post_metadata[post][2]
        if template != '':
            return template
    return default

# Return the template to use to process page: default if not found
def page_template(page,default):
    template = default
    if str(page) == 'archives.sst':
        template = 'ssst-archives.html'
    elif str(page) == 'tags.sst':
        template = 'ssst-tags.html'
    elif str(page) == 'categories.sst':
        template = 'ssst-categories.html'
    template_path = templates / template        
    if template_path.exists():
        return template
    else:
        return default

# Make the output (in dest_root) for the item (from src_root). The output
# replaces what is in the destination always (and dest_root_changes updated
# accordingly): the reason is that make is called if something in src is newer
# than the destination, so we have to make sure that the timestamp changes (even
# if the actual contents do not change) or else this item will be remade forever
#
# FIXME: THIS IS A LIE
# Media files are copied to the destination folder if new, changed, or newer
# than what is in the destination. Media files removed from the source are
# removed from the destination.
# return: list of changed files
def make_item(item: Path) -> list:
    log(1,'Processing ' + str(src_path_of(item)))
    # Clean the destination folder (only for posts that are guaranteed
    # have separate folders!)
    dest_folder = dest_folder_of(item)
    if (is_post(item) and dest_folder.exists()):
        clean_folder(dest_folder)
    # create a temorary directory to work in; will be deleted when with closes
    with tempfile.TemporaryDirectory() as tmpdir:
        # make comments, stores result in tmpdir/comments.html
        # FIXME: this also tries to make comments for auxiliary files
        commentcount = make_comments(item,tmpdir)
        if is_post(item):
            log(2,'Processing post')
            template = post_template(item,'ssst-post.html')
        else:
            log(2,'Processing page')
            template = page_template(item,'ssst-page.html')
        # make the item using the specified template, merging in comments
        # in tmpdir
        changed = make_item_merge_comments(item,commentcount, \
                            tmpdir + '/comments.html',template)
    # FIXME : return relative paths
    changed = changed + copy_media(item)
    return(changed)

# Make all items (posts, pages, and all generated .sst files)
# return a list of changed items
def make_items() -> list:
    changed = []
    # determine all items that need to be updated 
    for item in worklist():
        # make them and record all changed files in the destination
        changed = changed + make_item(item)
    return(changed)

# ---
# PRUNING
# ---

# Prune any auxiliary files that have been generated earlier but were not
# generated thios time
# - generated: list of auxiliary files generated this time
def prune_auxiliary_files (generated_files: list):
    existing_files = src_files_with_suffix('.sst')
    for f in existing_files:
        if f not in generated_files:
            log(3,"Pruning auxiliary file " + str(f))
            f.unlink()

# Prune any old items that have been generated earlier but for which the source
# no longer exist
def prune_html_files():
    # determine all .html files that exist, and schedule them for deletion
    # (unless the process below finds a source for them)
    dest_files = list(dest_files_with_suffix('.html'))
    # determine all sources for these html files that still exist
    src_files = list(src_files_with_suffix('.md')) + \
                list(src_files_with_suffix('.sst')) + \
                [src_path_of('feed/index.md')]  # dont delete feed!
    deleted = []
    for dest in dest_files:
        md_src = possible_src_of(dest,'.md')
        sst_src = possible_src_of(dest,'.sst')		
        if ((md_src not in src_files) and (sst_src not in src_files)):
            log(3,"Pruning destination file " + str(dest))
            dest.unlink()
            deleted.append(dest.relative_to(dest_root))
    return(deleted)

# ---
# MAKE SUMMARIES
# ---

# Regular expression matching img src's and href's that point to a local
# media object (i.e. paths that start with './' and then do not contain
# further slashes). A match returns two groups: 1) the src/href prefix and
# 2) the path without the leading "."
# (FRAGILE CODE: OTHER ITEMS MAY ALSO START WITH ./ )
localpath_re = re.compile(r'(src="|href=")\.(/[^/ ]+)"')

# Replacement to use when replacing local paths 
localpath_replacement = '' 

# filter replacing matched localpath with replacement text
# (using localpath_replacement)
# - matchobj: current match
def substitute_localpath(matchobj) -> str:
    if matchobj:
        # get the matched latex expression string
        prefix = matchobj.group(1)
        name = matchobj.group(2)        
        return(prefix + localpath_replacement + name + '"')


# Summarise a post and write the summary to a file. Summary is generated using
# pandoc with the specfied template. The length of
# the summary is determined by 'summary_length' (-z option). Fixes any links to
# media in the directory containing the post) by replacing them with their full
# path from the root of the destination tree (where the home page is where this
# summary will be included)
# (Note: make sure that this template does NOT generate a standalone HTML
# document; it is included in another page).
# - post: path to post to summarise
# - container: path to page that will contain this summary (used to fix urls)
# - date_format: date format string
# - template: template file name
# - fname: name of file to store summary in
def summarize_post( post: Path, container: Path, date_format: str \
                  , template: str, fname: str):
    # get string representation of date
    date = post_metadata[post][0]
    datestr = date.strftime(date_format)
    pandoc(post, fname, container, True, template, [], {}, datestr)
    # fix any local links in the output (that point to local media in the
    # directory containing the post) by replacing them with their full path
    # from the root of the destination tree (where the home page is where this
    # summary will be included)
    # FIXME: this assumes that summaries are only included in the home page
    global localpath_replacement
    localpath_replacement = str(post.parent)
    with open(fname,'r') as f:
        # read the file as one string
        orig_html = f.read()
    # replace all occurance of the regexp matching local links to media
    # with the their full path (using the prefix localpath_replacement)
    processed_html = localpath_re.sub(substitute_localpath, orig_html, count=0)
    # write the result back to the file
    with open(fname,'w') as f:
        f.write(processed_html)

# Make a summary page of some posts
# - posts: list of posts to summarise
# - date_format_str: format string to use to format dates in summaries
# - sub_template: template to use to generate one summary
# - template: template to generate the overall summaries page
# - chain: first post after posts summarised
# - src: src path (of driver .md file to structure summaries page)
# - dest: destination path to write the summaries page to (as html)
def summarize_posts( posts: list, date_format_str: str \
                   , sub_template:str, template: str \
                   , chain: Path, src_name: str, dest_name: str ):
    # creata a temorary directory to work in; will be deleted when with closes
    with tempfile.TemporaryDirectory() as tmpdir:
        # string keeping track of which temporary files to include later
        appends = []
        i = 0
        # creat a summary for the count most recent posts 
        for post in posts:
            # create a temporary html file name summarizing this entry
            i = i + 1
            summary_name = tmpdir + '/rss_feed.' + str(i) + '.html'
            # create the html summary for this entry
            # FIXME: assumes summaries are stored in file at root of the tree
            summarize_post( post, Path('index.html') \
                          , date_format_str \
                          , sub_template, summary_name)
            # and add this to the files to append when making the summary page
            appends.append(summary_name)
        # Create the main summary, including the summarized entries.    
        # The metadata in src will determine the title and subtitle
        dest = dest_path_of(dest_name)
        dest_tmp = dest.with_suffix('.tmp')
        # create destination folder if it doesnt exist yet
        os.makedirs(dest.parent, exist_ok=True)
        # get the current date and time in UTC and convert to full datestr
        now = datetime.datetime.today().astimezone(ZoneInfo('UTC'))
        datestr = now.strftime(rss_date_format_str)
        # execute pandoc to create the summaries html, merging
        # in the entry summaries generated earlier
        # FIXME: assumes summaries are stored in file at root of the tree
        metadata = { 'first' : str(item_url(chain,Path('index.html'))) }
        src = Path(src_name)
        pandoc(src, dest_tmp, src, False, template, appends, metadata, datestr)
        # replace file containing the home page only if its contents changed
        changed = replace_when_changed(dest,dest_tmp)    
        return(changed)

# ---
# MAKE HOME AND RSS FEED
# ---

# Make the home page, containing a summary of the five most revent posts
# Home page is generated using pandoc, with <templates>/ssst-homepage.html
# as template and <src>/index.md as markdown input
# return: whether home changed
def make_home() -> bool:
    log(0,'Making home page.')
    # if there are more than five posts, then append link to sixth post
    # FIXME: make chaining more useful
    if len(all_posts) > 5:
        chain = all_posts[5]
    else:
        chain = None ;
    result = summarize_posts( all_posts[0:5] \
                            , page_date_format_str \
                            , 'ssst-summary-entry.html' \
                            , 'ssst-homepage.html' \
                            , chain \
                            , 'index.md' \
                            , 'index.html' \
                            )
    return(result)


# Make the RSS feed
def make_rss_feed():
    log(0,'Making RSS feed.')
    result = summarize_posts( all_posts[0:10] \
                            , rss_date_format_str \
                            , 'ssst-rss-entry.html' \
                            , 'ssst-rss.html' \
                            , None \
                            , 'index.md' \
                            , 'feed/index.html' \
                            )
    return(result)


# ---
# REPORT CHANGES AND CREATE UPLOAD SCRIPT
# ---

# report all files in the list in the log
def report(l: list, m: str):
    if not l:
        log(0,'No files ' + m + ".")
    else:
        log(0,'Files ' + m + ":")
        for p in l:
            log(1,str(p))

# for each path in l, write the command to f, replacing the
# modifiers {path} and {file} in command with the
# parent folder and the filename of the path    
def write_script_commands ( l: list, command: str, f):
    for p in l:
        f.write(command.format(
            path = p.parent,
            file = p.name
        ))
        f.write('\n')
    
# Report changes and create uplaod script that uploads all changed
# files in the destination, and deletes all files no longer needed
# FIXME: use relative paths!!!
def process_changes (changes: list, deletions: list):
    # report changes
    report(changes,'changed in destination')
    report(deletions,'deleted in destination')    
    # create uploadscript (if requested); check for commands
    if (uploadscript != ''):
        if (uploadcommand == ''):
            error('No upload command string specified')
        if (deletecommand == ''):
            error('No delete command string specified')
        log(0,'Creating upload script')
        # now create the upload script
        with open(uploadscript,'w') as f:
            write_script_commands(changes,uploadcommand,f)
            write_script_commands(deletions,deletecommand,f)


# ---
# MAIN
# ---

# Set up arguments parsing
parser = argparse.ArgumentParser(description='SSST. A simple static site tool to maintain websites based on markdown and pandoc.')
parser.add_argument('-s', '--source', default=src_root,
                    help='Path to the root of the source files tree')
parser.add_argument('-d', '--destination', default=dest_root,
                    help='Path to the root of the destination files tree')
parser.add_argument('-r', '--root', default=hosting_root,
                    help='Root where the site is hosted. If omitted, the root is found using relative addressing, making the site relocatable')
parser.add_argument('-t', '--templates', default=templates,
                    help='Path to directory with templates')
parser.add_argument('-v', '--verbosity', default=loglevel,
                    help='Verbosity (-1, the default, is silent)')
parser.add_argument('-l', '--logfile', default='ssst.log',
                    help='Name of file to store all log messages in')
parser.add_argument('-u', '--uploadscript', default='',
                    help='Name of script file to store upload commands in')
parser.add_argument('-c', '--uploadcommand', default='',
                    help='Upload command string (with modifiers)')
parser.add_argument('-x', '--deletecommand', default='',
                    help='Delete command string (with modifiers)')
parser.add_argument('-f', '--force', default = False, action='store_true',
                    help='(Re)make everything')
parser.add_argument('-g', '--gladtex', default = False, action='store_true',
                    help='Use pandoc --gladtex to process LaTeX equations.')
parser.add_argument('-k', '--keepsimple', default = False, action='store_true',
                    help='Do not process simple LaTex equations')
parser.add_argument('-p', '--pedantic', default = False, action='store_true',
                    help='Abort after warning')
parser.add_argument('-z', '--summarylength', default=summary_length,
                    help='Number of lines in post (including YAML header) to include in a summary')
parser.add_argument('-o', '--overwrite', default = False, action='store_true',
                    help='Overwrite files even when contents unchanged')
args = parser.parse_args()

# Apply arguments and check validity
src_root = Path(args.source)
if not src_root.exists():
    error('Source tree %s does not exist.' % src_root)
dest_root = Path(args.destination)
if not dest_root.exists():
    error('Path to destination tree %s does not exist.' % dest_root)
hosting_root = args.root
templates = Path(args.templates)
if not templates.exists():
    error('Templates directory %s does not exist.' % templates)
loglevel = int(args.verbosity)
force = args.force
overwrite = args.overwrite
use_gladtex = args.gladtex
pedantic = args.pedantic
keep_simple_eqs = args.keepsimple
logname = args.logfile
uploadscript = args.uploadscript
uploadcommand = args.uploadcommand
deletecommand = args.deletecommand
summary_length = int(args.summarylength)

# open logfile
if logname != '':
    logfile = open(logname,'w')

# first collect all metadate (like tags and categories, but also title and date
# for all blog entries (posts and pages) in the source tree. This initializes
# - posts_with_cat (dictionary of all categories, listing all posts contained)
# - posts_with_tag (dictionary of all tags, listing all posts tagged as scuh)
# - post_metadata (dictionary of all posts, containing metadata
#   as tuples (date,title,template)
# - all_posts (essentially a copy of post_metadata): list of all relative
#   pathnames for posts, sorted by date (most recent first)
process_posts()

# Generate all auxiliary files, and return a list of all generated files
# - category files in ./category/<categorystring>/index.sst
# - tag files in ./tag/<tagstring>/index.sst
# - archive files in ./yyyy/mm/index.sst and
# - the overviews ./categories.sst and ./tags.sst and  ./archives.sst
#
# (make sure not to touch files whose contents do not change, so they do not
# show up as a worklist() item and will be skipped by make())
generated = generate_auxiliary_files()

# Prune any old auxiliary files that have been generated earlier but are no
# longer needed (eg because tag or vatergory no longert exists)
prune_auxiliary_files(generated)

# make all html files whose sources have changed (this includes the .sst files
# just generated), and store the list of changed items
changes = make_items()

# Prune any old items that have been generated earlier but for which a source
# no longer exist, and store the list of deleted items
deletions = prune_html_files()

# create the home page containing a summary of the most recent posts
# (mark changed if changed)
if make_home():
    changes.append(Path('index.html'))
    
# create the rss feed; the rss feed always changes (because it containt the
# current date)
make_rss_feed()
changes.append(Path('feed/index.html'))

# report all changed files and creat upload script
process_changes(changes,deletions)

        
# close the logfile if necessary
if logfile != None:
    logfile.close()


