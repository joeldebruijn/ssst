#!/usr/local/bin/python3
#
# SSST (version 2)
#
# A simple static site tool to maintain websites based on markdown and pandoc
#
# Author: Jaap-henk Hoepman (info@xot.nl)


# Note:
# - path: full path
# - name: basename of file

# ---
# IMPORTS
# ---

import argparse, sys, os, re, tempfile, subprocess, shutil, datetime, yaml, filecmp, stat
from pathlib import Path
from io import StringIO
from zoneinfo import ZoneInfo


# ---
# OPTIONS
# ---

# All items processed by SSST are *relative* paths to posts or pages, relative
# to the SRC_ROOT and DST_ROOT prefixes specified on the commandline
# and 'declared' as global variables here

# Path to the root of the source files tree
SRC_ROOT = Path('./src')

# Path to the root of the auxiliary files tree
AUX_ROOT = Path('./aux')

# Path to the root of the destination files tree
DST_ROOT = Path('./public')

# Root where the site is hosted
# (relative adressing, making the whole site relocatable, is the default)
HOSTING_ROOT = ''

# Path to directory with templates
TEMPLATES_FOLDER = Path('./templates')

# Abort after warning
PEDANTIC = False 

# Verbosity
LOGLEVEL = -1

# Force remaking all files
FORCE = False 

# Overwrite files even when contents are unchanged
OVERWRITE = False 

# Use pandoc --gladtex to process LaTeX equations
USE_GLADTEX = False

# Do not process simple LaTeX equations (pass them through in italics)
KEEP_SIMPLE_EQS = False

# Lenght of summaries (including YAML header)
SUMMARY_LENGTH = 20

# logfile name
LOGNAME = None 

# upload script file name
UPLOADSCRIPT = None 

# upload command for upload script, can containt the following modifiers
# {path} : replaced with the destination path of the changed item (without the
#    destination prefix specified by -d
# {src} : full path name of the changed file that needs to be uploaded
UPLOADCOMMAND = ''

# delete command for upload script, can containt the following modifiers
# {path} : replaced with the destination path of the changed item (without the
#    destination prefix specified by -d
# {src} : full path name of the changed file that needs to be uploaded
DELETECOMMAND = ''

def parse_commandline ():
    """Parse the commandline and set all options
    """
    global SRC_ROOT, AUX_ROOT, DST_ROOT, HOSTING_ROOT, TEMPLATES_FOLDER, LOGLEVEL
    global FORCE, OVERWRITE, USE_GLADTEX, PEDANTIC, KEEP_SIMPLE_EQS, LOGNAME
    global UPLOADSCRIPT, UPLOADCOMMAND, DELETECOMMAND, SUMMARY_LENGTH
    # Set up arguments parsing
    parser = argparse.ArgumentParser(description='SSST. A simple static site tool to maintain websites based on markdown and pandoc.')
    parser.add_argument('-s', '--source', default=SRC_ROOT,
                        help='Path to the root of the source files tree')
    parser.add_argument('-a', '--aux', default=AUX_ROOT,
                        help='Path to the root of the auxiliary files tree')
    parser.add_argument('-d', '--destination', default=DST_ROOT,
                        help='Path to the root of the destination files tree')
    parser.add_argument('-r', '--root', default=HOSTING_ROOT,
                        help='Root where the site is hosted. If omitted, the root is found using relative addressing, making the site relocatable')
    parser.add_argument('-t', '--templates', default=TEMPLATES_FOLDER,
                        help='Path to directory with templates')
    parser.add_argument('-v', '--verbosity', default=LOGLEVEL,
                        help='Verbosity (-1, the default, is silent)')
    parser.add_argument('-l', '--logfile', default='ssst.log',
                        help='Name of file to store all log messages in')
    parser.add_argument('-u', '--uploadscript', default='',
                        help='Name of script file to store upload commands in')
    parser.add_argument('-c', '--uploadcommand', default='',
                        help='Upload command string (with modifiers)')
    parser.add_argument('-x', '--deletecommand', default='',
                        help='Delete command string (with modifiers)')
    parser.add_argument('-f', '--force', default = False, action='store_true',
                        help='(Re)make everything')
    parser.add_argument('-g', '--gladtex', default = False, action='store_true',
                        help='Use pandoc --gladtex to process LaTeX equations.')
    parser.add_argument('-k', '--keepsimple', default = False, action='store_true',
                        help='Do not process simple LaTex equations')
    parser.add_argument('-p', '--pedantic', default = False, action='store_true',
                        help='Abort after warning')
    parser.add_argument('-z', '--summarylength', default=SUMMARY_LENGTH,
                        help='Number of lines in post (including YAML header) to include in a summary')
    parser.add_argument('-o', '--overwrite', default = False, action='store_true',
                        help='Overwrite files even when contents unchanged')
    args = parser.parse_args()
    # Apply arguments and check validity
    SRC_ROOT = Path(args.source)
    if not SRC_ROOT.exists():
        error(f'Source path { SRC_ROOT } does not exist.')
    AUX_ROOT = Path(args.aux)
    if not AUX_ROOT.exists():
        warning(f'Auxiliary path { AUX_ROOT } does not exist. Creating it.')
        os.makedirs(AUX_ROOT, exist_ok=False)
    DST_ROOT = Path(args.destination)
    if not DST_ROOT.exists():
        warning(f'Destination path { DST_ROOT } does not exist. Creating it.')
        os.makedirs(DST_ROOT, exist_ok=False)
    HOSTING_ROOT = args.root
    TEMPLATES_FOLDER = Path(args.templates)
    if not TEMPLATES_FOLDER.exists():
        error(f'Templates directory { TEMPLATES_FOLDER } does not exist.')
    LOGLEVEL = int(args.verbosity)
    FORCE = args.force
    OVERWRITE = args.overwrite
    USE_GLADTEX = args.gladtex
    PEDANTIC = args.pedantic
    KEEP_SIMPLE_EQS = args.keepsimple
    LOGNAME = args.logfile
    UPLOADSCRIPT = args.uploadscript
    UPLOADCOMMAND = args.uploadcommand
    DELETECOMMAND = args.deletecommand
    SUMMARY_LENGTH = int(args.summarylength)

# ---
# CONSTANTS
# ---

# Date format string constants
RSS_DATE_FORMAT_STR = '%a, %d %b %Y %H:%M:%S +0000'
PAGE_DATE_FORMAT_STR = '%B %-d, %Y'

# slack for determining whether file is older than another (in ms (?))
GRACE_PERIOD = 10

# Suffix of media files that will automatically be copied from the source
# folder to the destination folder
MEDIA_TYPES = ['jpg','gif','pdf','png']

# ---
# SHELL
# ---

def run_shell_command ( command: str ):
    """Run a shell command. Abort and print the error message when it fails.
       - command
    """
    log(4,f'Running shell command { command }.')
    result = subprocess.run(command,text=True,shell=True,capture_output=True)
    if result.returncode != 0:
        error(result.stderr)

# ---
# PANDOC PROCESSING
# ---

def pandoc ( src, dst, container: Path, summarize: bool \
           , template: str, appends: list, metadata: dict ):
    """Process a markdown source file to create the corresponding html
       destination. Output may be included in a different final html file with
       path container. Any references to local resources (./) are replaced
       with the appropriate link to the folder with the actual media.
       (This assumes that every source is always processed once locally, and
        its associated media is copied to its local destination folder.)

       - src : full path to item to process (either in src or in aux,
           could also be a comment file)
       - dst : full path of destination file
       - container : relative path of final html file this file may be included
           in (used to determine root and path metadata)
           relative path of src if standalone
       - summarize: if true, include only summary_length lines of the input
       - template: name of template to use
       - appends: other html files pandoc should append (-A)
       - metadata: dictionary of metadata items to include in the pandoc commandline
        (--metadata)
    """
    assert src.exists()
    # construct pandoc options (template, gladtex, appends, metadata
    options = f' --template={ get_template(template) }'
    if USE_GLADTEX:
        options += ' --gladtex '
    for append in appends:
        options += f' -A { append }'
    # collect metadata
    metadata['root'] = path_root(container) # root for this html file
    src_rel = None
    if src.is_relative_to(SRC_ROOT):
        src_rel = src.relative_to(SRC_ROOT)
    elif src.is_relative_to(AUX_ROOT):
        src_rel = src.relative_to(AUX_ROOT)
    # input to pandoc (markdown) always comes from either src or aux
    assert src_rel != None
    metadata['fullpath'] = src_rel.with_suffix('.html')
    # link to full html version of this file, relative to container
    path = path_url(src_rel,container)
    metadata['url'] = path        
    # append metadata string
    for m in metadata:
        options += f' --metadata { m }="{ metadata[m] }"'
    # first replace all occurances of ./ with the current base directory
    # (replacement should not contain &)
    base = str(Path(path).parent)
    command = f'cat { src } | sed "s&\\./&{ base }/&g"'
    # when summarizing, use head to get the first SUMMARY_LENGTH lines
    if summarize:
        command += f' | head -n { SUMMARY_LENGTH }'
    command += f' | pandoc -s -f markdown -t html { options } -o { dst }'        
    run_shell_command(command)

# ---
# PATH utility functions
# ---

def aux_path_of ( path: Path ) -> Path :
    """Return the full auxiliary pathname for a relative path.
    """
    return AUX_ROOT / path

def get_template ( name: Path ) -> Path :
    """Return the full path for a template, throw error if it does not exist
       - name: template file name to be found in the template directory
       - result: full path for the template; Path
    """
    template = TEMPLATES_FOLDER / name        
    if not template.exists():
        error(f'Template { template } not found.')
    return template

def path_relative_root ( relpath: Path ) -> str:
    """Return the relative path back to the root from the
       path containing the item.
       - item: relative path to item; Path
       - result: root (without a trailing backslash); str
    """
    # count the number of directory components
    hops = len(relpath.parts) - 1
    # chain as many .. together separated with / (avoid trailing /)
    path = '/'.join( hops * ['..'])
    if path == '':
        return "."
    else:
        return path
    
def path_root ( relpath: Path ) -> str:
    """Return the root for the current item on the hosting site.
       This is either a relative path back to the root from the
       path containing the item, or simply HOSTING_ROOT if set
       - item: relative path to item; Path
       - result: root (without a trailing backslash); str
    """
    if HOSTING_ROOT != '':
        return HOSTING_ROOT
    else:
        return path_relative_root(relpath)
    
def path_url ( relpath, context: Path ) -> str:
    """Obtain link pointing to a page (relpath) in the hosting site, when included
       in another page (context).
       Returns a relative link if HOSTING_ROOT is not set.
       - relpath: relative path to item (either in AUX_ROOT or in SRC_ROOT); Path
       - context: relative path to page in which link is included; Path
       - result: url; str
    """
    path = relpath.with_suffix('.html')
    if HOSTING_ROOT != '':
        return f'{ HOSTING_ROOT }/{ path }' 
    else:
        # get the common prefix of both parents
        prefix = os.path.commonpath((relpath.parent,context.parent))
        # remove the common prefix
        rel_context = context.relative_to(prefix)
        rel_path = path.relative_to(prefix)
        # now compute the relative root of context and append the path
        root = path_relative_root(rel_context)
        # TODO for 'compatibility' with ssst v1
        if root == '.':
            return rel_path
        else:
            return f'{ root }/{ rel_path }'

# --- interacting wiht the filesystem

def is_older(dest,src: Path) -> bool:
    """ Check whether dest is older than src
        - dest: ; Path
        - src: ; Path
        - result: ; bool
    """
    dtime = os.stat(dest)[stat.ST_MTIME]
    stime = os.stat(src)[stat.ST_MTIME]
    return dtime < (stime - GRACE_PERIOD)

def all_files_with_suffix( root: Path, suffix: str ):
    """Return an iterator that iterates over all files in the tree rooted at
       the specified root with the specified suffix
       - root: root of the tree to scan
       - suffix: suffix of files to return (must start with ".")
       - result: iterator
    """
    return root.glob('**/*' + suffix)
    
def delete_empty_folders ( root: Path ) -> list:
    """Delete all empty folders rooted in root, and report all that were deleted.
    """
    fpaths = list(all_files_with_suffix(root,''))
    # sort according to decreasing length
    # (to ensure folders that only contain empty folders are also deleted)
    fpaths.sort(key=lambda path: -len(str(path)))
    deleted = []
    for fpath in fpaths:
        if fpath.is_dir():
            if len(list(fpath.iterdir())) == 0:
                log(4,f'Deleting empty directory { fpath }.')
                fpath.rmdir() # delete
                deleted.append(fpath)
    return deleted

def is_obsolete_dst_file (f: Path, prefixes: set ) -> bool:
    """Test whether f is obsolete, is not covered by prefixes that have
       a source.

       If f is a file, then obsolete if
       - f.parent not in prefixes
       - f.parent / KEEP doesn't exists
       - f.parent does not equal the RSS feed path

       If f is a directory, then obsolete if
       - f not in prefixes
       - f / KEEP doesn't exist
       - f  does not equal the RSS feed path
    """
    if not f.is_dir():
        f = f.parent
    else:
    result  = (f not in prefixes) and (not (f / "KEEP").exists()) and \
                 (f !=  DST_ROOT / RSS_FEED_PATH.parent)
    log(1,f'Result: { result } ')
    return result

def delete_obsolete_dst_files ( src_items, aux_items : list ) -> list:
    """Delete all files in dst that no longer have a corresponding source,
       and return a list of all deleted files
       - src_items: list of items in src; [Item]
       - aux_items: list of items in aux; [Item]
       - result: list of deleted files; [path]
    """
    log(0,f'Deleting all obsolete files from {DST_ROOT}')
    # sanity check (to prevent deleting say the user home directory)
    canary =  DST_ROOT / 'ssst.canary'
    if canary.exists():
        # add all prefixes of all src_items and aux_items
        prefixes = set()
        for item in src_items + aux_items:
            prefix = item.dst_path().parent
            while prefix != Path('.'):
                prefixes.add(prefix)
                prefix = prefix.parent
        # now find all obsolete files in dst usng this  list of prefixes
        dst_files = all_files_with_suffix(DST_ROOT,'')
        to_delete = [ f for f in dst_files if is_obsolete_dst_file(f,prefixes) ]
        log(1,f'Deleting { len(to_delete) } obsolete files')
        # sort according to decreasing length
        # (to ensure folders that only contain empty folders are also deleted)
        to_delete.sort(key=lambda path: -len(str(path)))
        for fpath in to_delete:
            log(2,f'Deleting { fpath }')
            if fpath.is_dir():
                fpath.rmdir() # delete
            else:
                fpath.unlink() # delete
        return to_delete
    else:
        log(1,f'Not deleting anything because { canary } does not exist.')
        log(1,f'Create it manually in { DST_ROOT }.')
        return []
    
def replace_when_changed (current: Path, update: Path) -> bool:
    """ Replace the contents of a file with new contents, but only change the file
        and its metadata when the new contents are different from the current
        contents (or if OVERWRITE set).
        - current: file whose contents must be updated 
        - update: file containing new content
        - result: whether file changed
    """
    if (not current.exists()) \
       or OVERWRITE \
       or (not filecmp.cmp(current,update,shallow=False)): 
        log(5,'File changed ' + str(current))
        update.rename(current)
        return True
    else:
        log(5,'File not changed ' + str(current))
        update.unlink() # remove file
        return False

def filter_file ( inpath, outpath : Path, filter):
    """Filter file using filter function and write output (to possibly
       different file)
       - inpath: path to input
       - outpath: path to output
       - filter: filter function; str -> str
    """
    with open(inpath,'r') as inf:
        s = inf.read()
    s = filter(s)
    with open(outpath,'w') as outf:
        outf.write(s)
""
    
# ---
# ERRORS / LOGGING
# ---

# Outpur the error message, and log it when required. Then exit
# - msg: the error message
def error( msg: str ):
    m = f'ssst: error: { msg }\n'
    sys.stderr.write(m)
    if logfile != None:
        logfile.write(m)
    sys.exit (1)

# Output the warning message, and write it to the log file if it was specified.
# Exit when pedantic processing was specified.
# - msg: the warning message
def warning ( msg: str ):
    m = f'ssst: warning: { msg }\n'
    sys.stderr.write(m)
    if logfile != None:
        logfile.write(m)
    if PEDANTIC:
        sys.exit (1)

# Output a log message when its level is lower than the specifief verbosity
# on the command line. Always write the message to the log file if it was
# specified 
# - level: the log-level for this message
# - msg: the log message
def log ( level: int, msg: str ):
    if level == 0:
        m = f'# { msg }\n'
    else:
        m = f"{ '-' * level } { msg }\n"
    if (level <= LOGLEVEL):
        sys.stdout.write(m)
    if logfile != None:
        logfile.write(m)

# ===
# DelayedTextfile
# ===

# state of path as recorded in a snapshot
CHANGED = 0
UNCHANGED = 1
UNTOUCHED = 2

class DelayedTextfile ( StringIO ):
    """Create a textfile, whose contents is only actually written to external
       storage when closing the file and the new contents are different from
       the old contents (or when the file does not exist yet).
    """
    
    def __init__(self, fname, snapshot):
        """Create a Delayedtextfile for external file fname; use snapshot
           to record state changes.
        """
        super().__init__()
        self._path = Path(fname)
        self._snapshot = snapshot

    def close(self):
        newcontents = self.getvalue()
        if self._path.exists():
            with open(self._path,'r') as f:
                oldcontents = f.read()
                if (oldcontents == newcontents) and not OVERWRITE:
                    log(5,f'File not changed { self._path }.')
                    self._snapshot[self._path] = UNCHANGED
                    return
        # create parent directory; necessary if eg date in post
        # doesn't correspond with path, so archive file will be
        # written in an not-yet-existing path
        os.makedirs(self._path.parent, exist_ok=True)
        with open(self._path,'w') as f:
            f.write(newcontents)
            log(5,f'File changed { str(self._path) }.')
            self._snapshot[self._path] = CHANGED                    

    # Allow with DelayedTextfile(name) as f: constructions
    # where file is automatically closed when with statement ends
    # (NOTE SEPT 2022: APPARENTLY NO LONGER NEEDED)
    def __exit__(self, type, value, tb):
        pass 
        #self.close()

# ---
# YAML
# ---

def get_yaml_dict ( src: Path ) -> dict:
    """Return the dictionary of entries found in the first YAML block at
       the start of a file
       - src: full path to the file
    """
    try:
        with open(src,'r') as f:
            yamls = yaml.load_all(f,Loader=yaml.SafeLoader)
            for y in yamls:
                return y
        warning(f'Empty YAML block in {src}.')
        return {}
    except:
        warning(f'Invalid YAML block in {src}.')
        return {}

def ensure_list ( yaml_dict: dict, key: str ) -> list:
    """Return the value of key in yaml_dict always as a list (even if it
       is a single string).
    """
    if key in yaml_dict:
        value = yaml_dict[key]
        if type(value) is str:
            return [ value ]
        else:
            return value
    else:
        return []

def parse_date (s) -> datetime.date:
    """Parse a date if it is a string, return unchanced otherwise.
       (Only parses wordpress generated dates like
       'Sun, 19 Apr 2020 08:42:00 +0000'
       that the YAML parser fails to parse)
       - s: string to parse if it isn't a date
    """
    if type(s) is str:
        time = datetime.datetime.strptime(s,'%a, %d %b %Y %H:%M:%S %z')
        return time.date()
    else:
        return s
    
# ---
# Item
# ---

def capitalise ( s: str ) -> str:
    """Captilize first letter, but unly if the rest of the string is lower case
       (so iOS stays iOS, but normal terms like identity become Identity)
       - s: str
       - result: capitalied string
    """
    if (s[1:].lower() == s[1:]):
        return s[0].upper() + s[1:]
    else:
        return s

# Item types
POST = 0
PAGE = 1
AUX = 2
    
class Item:
    """Class representing an item (post, page, ..)
    """

    # Pointer to the list of all posts (maintained in sorted order, most recent
    # last; used to maintain prev/next pointes 
    head = None

    def _insert(self):
        """Insert yourself in the linked list of posts (maintained using
           self.prev and self.next, using Item.head as the start of this list,
           and keeping the lost ordered by increasing date
        """
        if (self.type != POST) or (self.date == None):
            self.prev = None
            self.next = None
        else:
            prev = None
            cur = Item.head
            while cur and cur.date and cur.date < self.date:
                prev = cur
                cur = cur.next
            # found spot; insert yourself
            self.prev = prev
            self.next = cur
            if cur:
                cur.prev = self
            if prev:
                prev.next = self
            else:
                self.next = Item.head
                Item.head = self
                
    def __init__(self, itempath: Path):
        """Load item from markdown file and set values of its fields to metadata
           in the file.
           - itempath: full path to markdown file
        """
        log(2,f'Processing item {itempath}.')
        # set relative path
        if itempath.is_relative_to(SRC_ROOT):
            self.relative_path = itempath.relative_to(SRC_ROOT)
            # If it is a POST, this will be adjusted at the end of init
            self.type = PAGE 
        elif itempath.is_relative_to(AUX_ROOT):
            self.relative_path = itempath.relative_to(AUX_ROOT)
            self.type = AUX
        else:
            error(f'Unexpected item path { itempath }.') 
        yaml_dict = get_yaml_dict(itempath)
        self.categories = ensure_list(yaml_dict,'categories')
        # Captilize first letter of a tag, but unly if the rest of
        # the tag is lower case
        self.tags = list( map( capitalise, ensure_list(yaml_dict,'keywords') ) )
        if 'title' in yaml_dict:
            self.title = yaml_dict['title']
        else:
            error(f'Item { itempath } is missing a title.')
        if 'date' in yaml_dict: 
            self.date = parse_date(yaml_dict['date'])
        else:
            self.date = None
        if 'template' in yaml_dict:
            self.template = yaml_dict['template']
        else:
            self.template = ''
        # pages and aux files do not have categories and tags
        if (len(self.categories) != 0) or (len(self.tags) !=0) :
            self.type = POST
            if len(self.categories) == 0:
                warning(f'Post {itempath} is missing a category.')
            if len(self.tags) == 0:
                warning(f'Post {itempath} is missing a keyword.')
            if self.date == None:
                error(f'Post {itempath} is missing a date.')
            # TODO warn if post path contains data, unequal to date in post
        self._insert()

    def yearmonth (self):
        """Return the date of an item disregarding the day
        """
        return datetime.date(self.date.year, self.date.month, 1)
        
    def src_path(self):
        """Return the full src path of item (either in aux or in src)
        """
        if self.type == AUX:
            return AUX_ROOT / self.relative_path
        else:
            return SRC_ROOT / self.relative_path

    def dst_path(self):
        """Return the destination path (with html suffix) where
           output for item will be generated in
        """
        return (DST_ROOT / self.relative_path).with_suffix('.html')

    def commentfiles(self):
        """Return a list of all commentfiles for the item.
        """
        return list(self.src_path().parent.glob('comment*.mdc'))

    def mediafiles (self):
        """Return all media files for item, i.e. those in its path that
           match MEDIA_TYPES
        """
        res = []
        for tp in MEDIA_TYPES:
            res = res + list(self.src_path().parent.glob('*.' + tp))
        return res

    def dst_mediafiles (self):
        """Return all media files for item, i.e. those in its path that
           match MEDIA_TYPES, in its destination
        """
        res = []
        for tp in MEDIA_TYPES:
            res = res + list(self.dst_path().parent.glob('*.' + tp))
        return res

    def dst_eqfiles (self):
        """Return all equation files for item, i.e. those in its path that
           match *.svg, in its destination
        """
        return list(self.dst_path().parent.glob('*.svg')) 
    
def scan_src_items () -> list:
    """Scan all pages and posts in src, load their metadata, and return as
       a list of items.
       result: [item]
    """
    log(0,'Scanning posts and pages.')
    return [ Item(fpath) for fpath in all_files_with_suffix(SRC_ROOT,'.md') ]

# ---
# GENERATE AUXILIARY FILES (ARCHIVES, TAGS, CATEGORIES
# ---

# --- sort keys

def item_sort_increasing_date_key ( item ):
    return item.date

def item_sort_decreasing_date_key ( item ):
    d = item.date
    return (-d.year,-d.month,-d.day)

def archive_sort_key(date):
    return (-date.year,date.month,0)

# --- dump

def dump_posts ( posts: list, aux_relpath: Path, subtitle, title, snapshot: dict ):
    """Create a markdown document into aux_relpath, with title as title and
       label as subtitle) containing a list of links to all posts in posts
       (in the order given).
       - posts: list of posts to dump; [item]
       - aux_relpath: relative path (in the AUX folder) of file to dump into; Path
       - subtitle: subtitle to use; str
       - title: title to use; str
       - postfix: string to append after all posts (for next/prev links); str
       - snapshot: keep track of changes; dict
    """
    aux_path = aux_path_of(aux_relpath)
    # make sure directory for output exists
    os.makedirs(aux_path.parent, exist_ok=True)
    log(3,f'Dumping to { aux_path }.')
    with DelayedTextfile(aux_path, snapshot) as outf:
        outf.write('---\n')
        outf.write(f"title: '{ title }'\n")
        outf.write(f"subtitle: '{ subtitle }'\n")        
        outf.write('---\n')        
        for post in posts:
            datestr = post.date.strftime(PAGE_DATE_FORMAT_STR)
            url = path_url(post.relative_path, aux_relpath) 
            outf.write(f'- [{ post.title }]({ url }) ({ datestr })\n')

# --- generate

def generate_tag_files ( src_items: list, snapshot: dict ):
    """Generate tag files containing all posts for a particular tag
       in ./tag/<tag>/index.sst in aux. Also create a list of all
       known tags in ./tags.sst. These are markdown files that
       will subsequently be converted to html through the make routine.

       Unchanged files keep modification times for make routine.
       Keep track of changes through snapshot.

       src_items: [item]
       snapshot: {fpath:state}
    """
    log(0,'Generating tag files.')
    # items can have more than one tag; convert set to list to prevent duplicates
    tags = list({ tag for item in src_items for tag in item.tags })
    tags.sort(key=str.lower)
    tags_path = aux_path_of('tags.sst')
    with DelayedTextfile(tags_path, snapshot) as tagsf:        
        tagsf.write('---\n')
        tagsf.write("title: 'Tags'\n")
        tagsf.write('---\n\n')
        # initialise firstchar so first header is always written to output
        current_firstchar = 'Z'
        for tag in tags:
            # output a header to the tags file for each alphabetical change
            if current_firstchar != tag[0].upper():
                current_firstchar = tag[0].upper()
                tagsf.write(f'\n## { current_firstchar }\n\n') 
            # write a link to the current tag page in the main tagsfile 
            tag_relpath = Path(f'tag/{ tag.lower() }/index.sst')
            url = path_url(tag_relpath,Path('.'))
            tagsf.write(f'- [{ tag }]({ url })\n')
            # write all entries for the current tag to its page
            tag_posts = [ item for item in src_items if tag in item.tags ]
            tag_posts.sort(key=item_sort_decreasing_date_key)
            dump_posts(tag_posts, tag_relpath, 'Tag', tag, snapshot)

def generate_category_files ( src_items: list, snapshot: dict ):
    """Generate category files containing all posts for a particular category
       in ./category/<category>/index.sst in aux. Also create a list of all
       known categories in ./categories.sst. These are markdown files that
       will subsequently be converted to html through the make routine.

       Unchanged files keep modification times for make routine.
       Keep track of changes through snapshot.
    
       src_items: [item]
       snapshot: {fpath:state}
    """
    log(0,'Generating category files.')
    # items can have more than one category; convert set to list to prevent duplicates
    cats = list({ cat for item in src_items for cat in item.categories })
    cats.sort()
    cats_path = aux_path_of('categories.sst')
    with DelayedTextfile(cats_path, snapshot) as catsf:        
        catsf.write('---\n')
        catsf.write("title: 'Categories'\n")
        catsf.write('---\n\n')
        for cat in cats:
            # write a link to the current category page in the main categories file 
            cat_relpath = Path(f'category/{ cat.lower() }/index.sst')
            url = path_url(cat_relpath,Path('.'))
            catsf.write( f'- [{ cat }]({ url })\n')
            # write all entries for the current category to its page
            cat_posts = [ item for item in src_items if cat in item.categories ]
            cat_posts.sort(key=item_sort_decreasing_date_key)
            dump_posts(cat_posts, cat_relpath, 'Category', cat, snapshot)

def generate_archive_files ( src_items: list, snapshot: dict ):
    """Generate archive files containing all posts for a particular month in 
       ./yyyy/mm/index.sst in aux. Also create a list of all known archives
       in ./archives.sst. These are markdown files that will subsequently be
       converted to html through the make routine.

       Unchanged files keep modification times for make routine.
       Keep track of changes through snapshot.
    
       src_items: [item]
       snapshot: {fpath:state}
    """
    log(0,'Generating archive files.')
    # get a set of all different year,month pairs (as dates) for which a post exists
    # and then turn it into a list, to avoid dupliactes
    yearmonths = list({ item.yearmonth() for item in src_items if item.type == POST })
    # sort the posts by *decreasing* year, but increasing month/day within
    # a year; this ensures that the main archive file lists the most
    # recent year first
    yearmonths.sort(key=archive_sort_key)
    archives_path = aux_path_of('archives.sst')
    with DelayedTextfile(archives_path, snapshot) as archivesf:        
        archivesf.write('---\n')
        archivesf.write("title: 'Archives'\n")
        # archivef.write("template: 'ssst-archives.html'\n")
        archivesf.write('---\n')
        # initialise current year so first year is always written to output
        current_year = 0
        for i in range(len(yearmonths)):
            ym = yearmonths[i]
            # output a header to the main archives file for each year change
            if current_year != ym.year:
                current_year = ym.year
                archivesf.write(f'\n## { current_year }\n')
            # write a link to the current yearmonth page in the main archives file 
            ym_relpath = Path(ym.strftime('%Y/%m/index.sst'))
            ym_str = ym.strftime('%B %Y')
            ym_url = path_url(ym_relpath,Path('.'))
            archivesf.write(f'- [{ ym_str }]({ ym_url })\n')
            # write all entries for the current yearmonth to its page
            # (make sure only to include posts)
            ym_posts = [ item for item in src_items if (item.type == POST) and (ym == item.yearmonth()) ]
            ym_posts.sort(key=item_sort_increasing_date_key)
            # TODO add next and previous archive links to template
            dump_posts(ym_posts, ym_relpath, 'Archive', ym_str, snapshot)

def generate_aux_items ( src_items: list ) -> list:
    """Generate all auxiliary items (archives, tags, categories) in aux from
       the metadata in src_items (unchanged files keep modification times
       for make); obsolete files are deleted.
       Report all auxiliary items (including unchanged).
       src_items: [item]
       result: [item]
    """
    log(0,'Generating auxiliary data files.')
    log(1,f'Processing { len(src_items) } in the source tree.')
    postcount = len([item for item in src_items if item.type == POST])
    pagecount = len([item for item in src_items if item.type == PAGE])
    log(1,f'{ postcount } posts and { pagecount } pages.')
    # keep track of untouched files in the aux file tree
    # note: this snapshot also contains directories!
    # note: DIFFERENT Path objects for the SAME path string are equal
    # and are mapped to the same dictionary key
    snapshot = { fpath : UNTOUCHED for fpath in all_files_with_suffix(AUX_ROOT,'sst') }
    generate_tag_files(src_items, snapshot)
    generate_category_files(src_items, snapshot) 
    generate_archive_files(src_items, snapshot)
    # delete untouched files (i.e those that were never a target in the above steps)
    untouched = [ fpath for fpath in snapshot if snapshot[fpath] == UNTOUCHED ]
    for fpath in untouched:
        if not fpath.is_dir():
            fpath.unlink() # delete
    # now delete all empty direcotories in AUX
    delete_empty_folders(AUX_ROOT)
    # return all auxiliary items
    log(0,'Scanning auxiliary files.')
    return [ Item(fpath) for fpath in snapshot if snapshot[fpath] != UNTOUCHED ]


# ---
# LaTeX EQUATIONS
# ---

# some global variables for the equation processing routines

# number of matches so far
eq_match_count = 0

# folder containing the currently processed file
basefolder = ''

# dictonary of processed equations, containing for each equation the
# name of the SVG file containing its image;
# reset for every file
equations = {}

# list of svg file names that changed
changed_svgs = []

# whether to reuse already generated equation images for the same file
# that is currently processed
reuse = False

def process_eq(latex_expr):
    """Turn a latex expression into an SVG graphic and write it to file.
       The filename to use is determined by the global variable basefolder
       (derived from the parent of the file containing the equations) and
       number of processed equations for that file
       - latex_expr: latex expression (without surrounding $$)
       - returns: the name of the SVG file in which the graphic is stored
    """
    global eq_match_count, changed_svgs
    # increment match count and create corresponding SVG output name
    eq_match_count = eq_match_count + 1
    svg_file_name = f'index.{ eq_match_count }.svg'
    svg_path_name = basefolder / svg_file_name
    if reuse:
        # return path to earlier generated equation image (uses the fact
        # that files are always processed in exactly the same way, except
        # that summaries may only process the first part of it)
        log(4,'Reusing equation in ' + str(svg_path_name))
        return svg_path_name
    else:
        log(4,'Storing equation in ' + str(svg_path_name))
        # creata a temorary direcotry to work in; will be deleted when with closes
        with tempfile.TemporaryDirectory() as tmpdir:
            tmp_tex = tmpdir + '/htsv.tex'    
            tmp_pdf = tmpdir + '/htsv.pdf'
            tmp_svg = tmpdir + '/htsv.svg'
            # read the latex template and store it as a string in t
            eq_template = get_template('ssst-eq.tex')
            # insert latex_expr at $$ in the template
            filter_file(eq_template,tmp_tex \
                       ,lambda s: s.split('$$')[0] + '$' + latex_expr + '$' + s.split('$$')[1])
            # create the pdf (in the temporary directory) from the latexfile
            run_shell_command(f'pdflatex -output-directory {tmpdir} {tmp_tex}')
            # create the SVG from the pdf
            run_shell_command(f'pdf2svg { tmp_pdf } { tmp_svg }')
            # replace destination only when real changes occured
            replace_when_changed(svg_path_name,Path(tmp_svg))
            changed_svgs.append(svg_path_name)
            # return 'local' link not the full relative path
            return svg_file_name

# Match &lt;
lessthan_re = re.compile(r'(&lt;)')

# Match &gt;
greaterthan_re = re.compile(r'(&gt;)')

def sanitize_eq (expr):
    """Replace any occurance of '&lt;' on the input with '<'
       (because the --gladtex option of pandoc turns < into &lt;)
    """
    tmp = lessthan_re.sub('<',expr,count=0)
    ret = greaterthan_re.sub('>',tmp,count=0)    
    log(5,'Sanitized '+ expr + ' to ' + ret)
    return ret

# Match a simple expression, that can be passed without processing
simple_expr_re =  re.compile(r'([A-Za-z0-9])+')

def substitute_eq (matchobj):
    """Process currently matched latex equation to create an SVG for it
       return the replacement text for the currently matched latex equation
       (ie an <img> tag pointing the SVG file generated for it)
       If the exact same equation was already made, immediately return the
       <img> tag pointing to the SVG file previously made.
        - matchobj: object found to match the equation regular expression
        - result: replacement <img> tag
    """
    if matchobj:
        # get the matched latex expression string
        match = matchobj.group(1)
        # sanitize expressions
        latex_expr = sanitize_eq(match)
        log(4,'Processing equation ' + latex_expr)
        # do not process simple LaTex equations, when so instructed
        if KEEP_SIMPLE_EQS and simple_expr_re.fullmatch(latex_expr):
            return '<em>' + latex_expr + '</em>'
        if latex_expr in equations:
            log(5,'Already made!')
            svg_path = equations[latex_expr]
        else:
            svg_path = process_eq(latex_expr)
            equations[latex_expr] = svg_path
        # return the <img> tag that points to the generated SVG in the local dir
        return f'<img class="latexeq" src="{ svg_path }" alt="{ latex_expr }">'
    
# Match <eq ...>LATEXEXPRESSION</eq> generated by pandoc -gladtex
# (use .*? for non-greedy matching, to ensure the first </eq> found
# will close the regulear expression)
gladtex_eq_re = re.compile(r'<eq[^>]*>(.*?)</eq>',re.DOTALL)

# Match <span class="math inline">$LATEXEXPRESSION$</span> generated by
# pandoc without any math processing option.
# (use .*? for non-greedy matching, to ensure the first $</span> found
# will close the regulear expression)
plain_eq_re = re.compile(r'<span class="math inline">\$(.*?)\$</span>',re.DOTALL)

def filter_equation ( s: str ) -> str:
    """Fliter any equation in a string, matching <x>_eq_re
    """
    if USE_GLADTEX:
        return gladtex_eq_re.sub(substitute_eq, s, count=0)
    else:
        return plain_eq_re.sub(substitute_eq, s, count=0)

def process_equations ( src, eq_path: Path, snapshot: dict):
    """Process any equations in the item.
       - src: full path of item to be processed
       - eq_path: relative path of folder in which earlier generated equations
           are stored (None if they must be generated)
       - snapshot: to keep track of changed files
    """
    global eq_match_count, basefolder, equations, changed_svgs, reuse
    # 
    log(2,'Processing equations.')
    eq_match_count = 0
    equations = {}
    changed_svgs = []
    reuse = (eq_path != None) 
    if reuse:
        basefolder = eq_path
    else:
        basefolder = src.parent
    filter_file(src, src, filter_equation)
    for svg_name in changed_svgs:
        snapshot[Path(svg_name)] = CHANGED

# ---
# MAKE SUMMARIES
# ---

def summarize_posts( posts: list, container: Path, date_format_str: str \
                   , sub_template, template, dst_path: str, snapshot: dict ):
    """Make a summary page of some posts
       - posts: list of items to summarise
       - container: src path (of driver .md file to structure summaries page)
       - date_format_str: format string to use to format dates in summaries
       - sub_template: template to use to generate one summary
       - template: template to generate the overall summaries page
       - dst_path: destination path to write the summaries page to (as html)
       - snapshot: dictionary to keep track of changes ; { path }
    """
    # creata a temorary directory to work in; will be deleted when with closes
    with tempfile.TemporaryDirectory() as tmpdir:
        # keep track of which temporary files to include later
        i = 0
        summary_fnames = []
        # creat a summary for each of the posts in a separate tmp file 
        for post in posts:
            i += 1
            summary_fname = f'{ tmpdir }/summary.{ i }.html'
            metadata = { 'date' : post.date.strftime(date_format_str) }
            pandoc( post.src_path(), summary_fname, container, True \
                  , sub_template, [], metadata)
            # Reuse already generated equations in post.relative_path.parent
            process_equations(summary_fname, post.relative_path.parent, snapshot)
            summary_fnames.append(summary_fname)
        # Create the main summary page now, including the summarized entries.
        # The metadata in container will determine the title and subtitle
        src = SRC_ROOT / container
        dst = DST_ROOT / dst_path
        # create destination folder if it doesnt exist yet
        os.makedirs(dst.parent, exist_ok=True)
        # get the current date and time in UTC and convert to full datestr
        now = datetime.datetime.today().astimezone(ZoneInfo('UTC'))
        datestr = now.strftime(date_format_str)
        metadata = { 'date' : datestr}
        prev = posts[len(posts)-1].prev
        if prev:
            metadata['prev'] = path_url(prev.relative_path,Path(container))
        # execute pandoc to create the summaries html, merging
        # in the entry summaries generated earlier
        dst_tmp = dst.with_suffix('.tmp')
        pandoc(src, dst_tmp, container, False, template, summary_fnames, metadata )
        if replace_when_changed(dst,dst_tmp):    
            snapshot[dst] = CHANGED
        else:
            snapshot[dst] = UNCHANGED

# ---
# MAKE HOME AND RSS FEED
# ---

HOME_PATH = Path('index.html')

def make_home ( posts: list, snapshot: dict ):
    """Make the home page, containing a summary of the five most revent posts
       Home page is generated using pandoc, with <templates>/ssst-homepage.html
       as template and <src>/index.md as markdown input
       - posts: posts sorted, most recent first
       - snapshot: dictionary to keep track of changes ; { path }
    """
    log(0,'Making home page.')
    summarize_posts( posts[0:5], Path('index.md'), PAGE_DATE_FORMAT_STR \
                   , 'ssst-summary-entry.html', 'ssst-homepage.html' \
                   , HOME_PATH, snapshot)

RSS_FEED_PATH = Path('feed/index.html')

def make_rss_feed ( posts: list, snapshot ): 
    """Make the RSS feed from posts (sorted).
       - posts: posts sorted, most recent first
       - snapshot: dictionary to keep track of changes ; { path }
    """
    log(0,'Making RSS feed.')
    # The 'container' is actually the root index file as links in rss feeds
    # always must be absolute paths with the actual hosting root in them; the
    # template file shoudld deal with this)
    summarize_posts( posts[0:10], Path('index.md'), RSS_DATE_FORMAT_STR \
                   , 'ssst-rss-entry.html', 'ssst-rss.html' \
                   , RSS_FEED_PATH, snapshot)
# ---
# MAKING ITEMS
# ---

def item_template ( item: Item, default: str ) -> str:
    """Return the template to use to process item: default if none specified
       in the YAML header for item.
    """
    if item.template != '':
        return item.template
    return default

def aux_template ( aux_item: Item, default: str ) -> str:
    """Return the template to use to process an aux_item: default if not found.
    """
    template = default
    aux_relpath = aux_item.relative_path
    if str(aux_relpath) == 'archives.sst':
        template = 'ssst-archives.html'
    elif str(aux_relpath) == 'tags.sst':
        template = 'ssst-tags.html'
    elif str(aux_relpath) == 'categories.sst':
        template = 'ssst-categories.html'
    template_path = TEMPLATES_FOLDER / template        
    if template_path.exists():
        return template
    else:
        return default

def copy_media ( item: Path, snapshot: dict ):
    """Copy all media associated with an item, keeping their metadata.
       And keeping track of changes through snapshot.
       - item: path to item in the source tree
    """
    log(2,'Copying mediafiles')
    dst_folder = item.dst_path().parent
    for fpath in item.mediafiles():
        log(4,f'Copying media file { fpath } to { dst_folder }.')
        shutil.copy2(fpath, dst_folder) # keep metadata
        dstpath = DST_ROOT / fpath.relative_to(SRC_ROOT)
        snapshot[dstpath] = CHANGED

# --- comments

# The code below assumes that comments for an item are stored in the same
# folder as the item itself, with the following structore
# ./comment.1.mdc (first comment)
# ./comment.1.1.mdc (first comment to first comment
# ./comment.1.2.mdc (second comment to first comment
# ./comment.2.mdc (second comment)
# etc., with arbitrary nesting allowed

def fold_comments ( path: str ):
    """Assume all comment files have been converted to html and are stored in
       the temporary directory. 
       Recursively fold all coments at the current level (specified by path in
       the temporary directory)
       and return them as a string; try: <path>.1.html (and fold its children)
       then  <path>.2
    """
    log(5,f'Folding { path }.')
    folded = ''
    i = 1
    filename = f'{ path }.{ i }.html'
    while os.path.exists(filename):
        folded_children = fold_comments(f'{ path }.{ i }')
        with open(filename,'r') as f:
            folded += '<div class="comment">\n'
            folded += f'{ f.read() }\n{ folded_children }\n</div>\n'
        i += 1
        filename = f'{ path }.{ i }.html'
    return folded

def make_comments( item: Item, tmpdir: Path ) -> int:
    """ Process the comments associated with the item. 
        - first convert individual comments to html in tmpdir
        - then fold them into one html file (keeping the hierarchical order)
        Stores the result in tmpdir/comments.html for further processing
        by make (this file is empty if there are no comments)

        - item: item to process comments for; Item
        - tmpdir: tmp folder to store tmp files and resulting comments.html in
        - result: number of comments
    """
    log(3,'Processing comments')
    commentfiles = item.commentfiles()
    # first convert all .mdc files in src_root to .html files in tmpdir
    for c in commentfiles:
        # filename to store html converted comment in (in tmpdir)
        outf = f'{ tmpdir }/{ c.name[:-4] }.html'
        # blog-post path and comment depth indicator for reply links
        # strip mdc from the back
        reply_path = str(c.relative_to(SRC_ROOT))[:-3]
        metadata = { 'reply' : reply_path }
        log(4,f'Making comment HTML { outf }.')
        pandoc(c, outf, item.relative_path, False, 'ssst-comment.html', [], metadata )
    # now recursively fold in comments created in tmpdir
    # assumes all commentfiles are of the form comment.*.html where * is secquence of indices
    comments = fold_comments(tmpdir + '/comment')
    log(5,comments)
    # and store the comments in comments.html in the tmpdir
    with open(tmpdir + '/comments.html','w') as f:
        f.write(comments)
    return len(commentfiles)

# --- items

def make_item_merge_comments ( item, commentcount, comments, template, snapshot ):
    """Make the html item from the md or sst source specified by item, merging
       in any comments, and using item_template as the template
       return: list of changed svg files and the destination of the item itself
       - item: item to make; Item
       - commentcount: number of comments for item; int
       - comments: full pathname to html file containing comments to merge; str
       - template: full pathname of template file to use; str
       - snapshot: dictionary to keep track of changes ; { path }
    """
    src = item.src_path()
    dst = item.dst_path()
    # create destination folder if it doesnt exist yet
    os.makedirs(dst.parent, exist_ok=True)
    # build the metadata dictionary
    metadata = {}
    appends = []
    # TODO : AUX items can also have next/prev; adjust templates
    if item.next:
        metadata['next'] = path_url(item.next.relative_path,item.relative_path)
    if item.prev:
        metadata['prev'] = path_url(item.prev.relative_path,item.relative_path) 
    if item.type != AUX:
        # path for reply links: relative path of parent of item + 'comment.'
        metadata['reply'] = str(item.relative_path.parent) + '/comment.'
        if commentcount > 0:
            metadata['commentcount'] = str(commentcount)
            appends = [ comments ]
        if item.date:
            datestr = item.date.strftime(PAGE_DATE_FORMAT_STR)
            metadata['date'] = datestr
    pandoc(src, dst, item.relative_path, False, template, appends, metadata)
    # generated images for any complex equations in dst
    process_equations(dst, None, snapshot)
    snapshot[dst] = CHANGED

def make_item ( item: Item, snapshot: dict ):
    """Make the item (in src/aux) and store output in dst.
       Process any comments and copy any media.

       (The output always replaces what is in the destination: the reason is
       that make is called if something is newer than the destination,
       so we have to make sure that the timestamp changes (even if the actual
       contents do not change) or else this item will be remade forever.

       - item: item to make; Item
       - snapshot: dictionary to keep track of changes ; { path }
    """
    log(1,f'Making { item.relative_path }.')
    # create a temorary directory to work in; will be deleted when with closes
    with tempfile.TemporaryDirectory() as tmpdir:
        if item.type == POST:
            log(2,'Processing post')
            template = item_template(item, 'ssst-post.html')
        elif item.type == PAGE:
            log(2,'Processing page')
            template = item_template(item, 'ssst-page.html')
        else:
            log(2,'Processing auxiliary item')
            template = aux_template(item,'ssst-page.html')
        if item.type != AUX: 
            # make comments, stores result in tmpdir/comments.html
            ccount = make_comments(item, tmpdir)
            log(3,f'{ccount} comments found.')
        else:
            ccount = 0
        cpath = tmpdir + '/comments.html'
        # make the item using the specified template, merging in comments
        # in tmpdir
        make_item_merge_comments(item, ccount, cpath, template, snapshot)
        copy_media(item, snapshot)

def make_items ( workset: set ) -> (list,list):
    """Make every item in workset and keep track of changed and obsolete items
       in dst; delete obsolete items.
       workset: set of items to make: {item}
       result: changed and deleted items: [item],[item] 
    """
    log(0,'Making items...')
    # keep track of changed/deleted files in dst, including media files and equation files
    workset_paths = [ item.dst_path() for item in workset ]
    workset_paths += [ path for item in workset for path in item.dst_mediafiles() ]
    workset_paths += [ path for item in workset for path in item.dst_eqfiles() ]
    snapshot = { fpath : UNTOUCHED for fpath in workset_paths }
    # make each item in the workset (and keep track of changes through snapshot
    for item in workset:
        # ignore home (made separately) and KEEP.md (indicating that a folder
        # should not be deleted)
        if item.relative_path != Path('index.md'):
            make_item(item,snapshot)
    # make the RSS feed and the home page (using the most recent posts)
    posts = [item for item in src_items if item.type == POST]
    posts.sort(key=item_sort_decreasing_date_key)
    make_home(posts,snapshot)
    make_rss_feed(posts,snapshot)
    # clean up and report changes/deletions
    changed = [ fpath for fpath in snapshot if snapshot[fpath] == CHANGED ]
    to_delete = [ fpath for fpath in snapshot if snapshot[fpath] == UNTOUCHED ]
    for fpath in to_delete:        
        if not fpath.is_dir():
            log(5,f'Deleting { fpath }.')
            fpath.unlink() # delete
    # also delete all empty direcotories in DST and add them to report
    to_delete += delete_empty_folders(DST_ROOT)
    return ( changed, to_delete )
    
# ---
# DETERMINING WORK
# ---

def needs_work (item: Item) -> bool:
    """Determine whether item needs to be (re)made. True if
       - destination does not exist
       - destination exists but is older than source
       - destination exists but some comment or media file is older
       - destination exists but does not contain same mediafiles as source

       (also if item = home to ensure all associated files (equations) are
        added to snapshot in make)
    
       item: item to test ; Item
       return: whether it needs to be made ; bool
    """
    log(2,f'Considering { item.relative_path } for work.')
    dst = item.dst_path()
    log(4,f'Destination will be { dst }.')
    # check if home
    if item.relative_path == Path("index.md"):
        log(4,'Is home.')
        return True
    # check if dst exists
    if not dst.exists():
        log(4,'Destination does not exists.')
        return True
    # check whether destination is older than source
    src = item.src_path()
    if is_older(dst,src):
        log(4,'Destination older than source.')
        return True
    # check whether destination is older than any comment or mediafile in src
    files = item.commentfiles() + item.mediafiles()
    for file in files:
        log(4,f'Additional file found: { file }.')
        if is_older(dst,file):
            log(4,'Destination older than comment or media item in source.')
            return True
    # check whether destination does not contain same mediafiles as source
    src_mf = { mf.name for mf in item.mediafiles() }
    dst_mf = { mf.name for mf in item.dst_mediafiles() }    
    if src_mf != dst_mf:
        log(4,'Destination and source do not contain the same media item(s).')
        return True
    log(4,'Not a work item.')
    return False

def workset_for ( items: list ) -> list:
    """Return the subset of items that need to be made.
       items: list of all items; [item]
       result: list of workitems; [item]    
    """
    return { item for item in items if (FORCE or needs_work(item)) }

def determine_work ( src_items, aux_items: list ) -> set:
    """Determine the set of items to make in src_items and aux_items
       src_items: all items (posts, pages) in src; [item]
       aux_items: all auxiliary items in aux; [item]
       result: set of items to make;  {item}
    """
    log(0,'Determining items to make...')
    src_workset = workset_for(src_items) 
    log(1,f'Found { len(src_workset) } work items in source tree.')
    # add any previous or next items of items to the workset
    # (to ensure their next/prev links are updated)
    prevs = { item.prev for item in src_workset if item.prev != None}
    nexts = { item.next for item in src_workset if item.next != None}
    aux_workset = workset_for(aux_items)
    log(1,f'Found { len(aux_workset) } work items in auxiliary tree.')
    workset = src_workset | prevs | nexts | aux_workset
    log(0,f'Found { len(workset) } items to make.')
    return workset


# ---
# PROCESS CHANGES
# ---

def report ( paths: list, m: str ):
    """Report all files in a list in the log, using m as message
       - paths: list of paths to report; [path]
       - m: message; str
    """
    if not paths:
        log(0,f'No files { m }.')
    else:
        log(0,f'Files { m }:')
        for p in paths:
            log(1,str(p))

def write_script_commands ( paths: list, command: str, f):
    """For each path in l, write the command to f, replacing the
       modifiers {path} and {file} in command with the
       parent folder and the filename of the path
       - paths: list of paths to which command must be applied; [path]
       - command: command (with modifiers); str
       - f: file descriptor to write output to
    
    """
    for p in paths:
        f.write(command.format(
            path = p.parent,
            file = p.name
        ))
        f.write('\n')

def process_changes (changes: list, deletions: list):
    """Report changes and create uplaod script that uploads all changed
       files in the destination, and deletes all files no longer needed.
       - changes: list of files changed (full paths); [path]
       - deletions: list of files deleted (full paths); [path]    
    """
    # report changes
    report(changes,'changed in destination')
    report(deletions,'deleted in destination')    
    # create uploadscript (if requested); check for commands
    if (UPLOADSCRIPT != ''):
        if (UPLOADCOMMAND == ''):
            error('No upload command string specified')
        if (DELETECOMMAND == ''):
            error('No delete command string specified')
        log(0,'Creating upload script')
        # now create the upload script
        with open(UPLOADSCRIPT,'w') as f:
            write_script_commands(changes,UPLOADCOMMAND,f)
            write_script_commands(deletions,DELETECOMMAND,f)

# ---
# MAIN
# ---

# parse the command line, and set all options; aborts if any errors
parse_commandline() 
# open the logfile if neccesary
if LOGNAME != '':
    logfile = open(LOGNAME,'w')
# scan all pages and posts in src and load their metadata
src_items = scan_src_items()
# use this metadata to update all auxiliary files (archives, tags, categories)
# in aux (unchanged files remain untouched; obsolete files are deleted)
aux_items = generate_aux_items(src_items)
# convert all pages, posts and auxiliary items to their corresponding html
# files in dst; return a list of changed and a list of deleted html files
workset = determine_work(src_items,aux_items)
(changed,deleted) = make_items(workset)
# delete all obsolete files
deleted += delete_obsolete_dst_files(src_items,aux_items)
# report all changed files and create upload script 
process_changes(changed,deleted)
# close the logfile if necessary
if logfile != None:
    logfile.close()

    
    



