#!/usr/local/bin/python3
#
# SSST
#
# A simple static site tool to maintain websites based on markdown and pandoc
#
# Author: Jaap-henk Hoepman (jhh@xs4all.nl)

import argparse, sys, os, re, tempfile, subprocess, shutil, datetime, yaml, filecmp
from pathlib import Path
from io import StringIO

# ---
# DelayedTextfile
#
# Create a textfile, whose contents is only actually written to external
# storage when closing the file and the new contents are different from
# the old contents (or when the file does not exist yet)
# ---

class DelayedTextfile(StringIO):

    # create a Delayedtextfile for external file fname,
    # to be written on file close
    def __init__(self,fname):
        super().__init__()
        self.path = Path(fname)

    # return true if the file was actually written, false otherwise    
    def close(self):
        newcontents = self.getvalue()
        if self.path.exists():
            with open(self.path,'r') as f:
                oldcontents = f.read()
                if oldcontents == newcontents:
                    return False
        with open(self.path,'w') as f:
            f.write(newcontents)
            return True

    # Allow with DelayedTextfile(name) as f: constructions
    # where file is automatically closed when with statement ends
    def __exit__(self, type, value, tb):
        self.close()

# ---
# CONSTANTS / PARAMETERS
# ---

# All items processed by SSST are *relative* paths to posts or pages, relative
# to the src_root and dest_root prefixes specified on the commandline
# and 'declared' as global variables here

# Path to the root of the source files tree
src_root = Path('./src')

# Path to the root of the destination files tree
dest_root = Path('./public')

# Root where the site is hosted
# (relative adressing, making the whole site relocatable, is the default)
hosting_root = ''

# Path to directory with templates
templates = Path('./templates')

# Verbosity
loglevel = -1

# Force remaking all files
force = False 

# Use pandoc --gladtex to process LaTeX equations
use_gladtex = False

# Abort after warning
pedantic = False 

# Do not process simple LaTeX equations (pass them through in italics)
keep_simple_eqs = False

# Lenght of summaries (including YAML header)
summary_length = 20

# Dictionary of all tags and relative pathnames for the posts with those tags
# (filled by process_posts -> process_post_metadata) 
items_with_tag = {}

# Dictionary of all categories relative pathnames for the posts with those categories
# (filled by process_posts -> process_post_metadata) 
items_with_cat = {}

# Dictionary of all posts metadata (as tuples (date,title,template)) indexed
# by the relative pathname for the post
# (filled by process_posts -> process_post_metadata) 
post_metadata = {}

# List of all relative pathnames for posts, sorted by date (most recent first)
# (with the help of process_posts)
all_posts = []

# logfile
logfile = None 

# ---
# ERRORS / LOGGING
# ---

# Outpur the error message, and log it when required. Then exit
# - msg: the error message
def error(msg: str):
    m = "ssst: error: %s\n" % str(msg)
    sys.stderr.write(m)
    if logfile != None:
        logfile.write(m)
    sys.exit (1)

# Output the warning message, and write it to the log file if it was specified.
# Exit when pedantic processing was specified.
# - msg: the warning message
def warning(msg: str):
    m = "ssst: warning: %s\n" % str(msg)
    sys.stderr.write(m)
    if logfile != None:
        logfile.write(m)
    if pedantic:
        sys.exit (1)

# Output a log message when its level is lower than the specifief verbosity
# on the command line. Always write the message to the log file if it was
# specified 
# - level: the log-level for this message
# - msg: the log message
def log (level: int, msg: str):
    if level == 0:
        m = '# '
    else:
        m = '-' * level + ' '
    m = m + str(msg) + '\n'
    if (level <= loglevel):
        sys.stdout.write(m)
    if logfile != None:
        logfile.write(m)


# ---
# SHELL
# ---

# Run a shell command. Abort and print the error message when it fails
# - command
def run_shell_command(command: str):
    log(4,'Running shell command ' + command)
    result = subprocess.run(command,text=True,shell=True,capture_output=True)
    if result.returncode != 0:
        error(result.stderr)
    
# ---
# YAML
# ---

# Return the dictionary of entries found in the YAML block at the start of
# a file
# - src: full path to the file
def get_yaml_dict(src: Path):
    try:
        with open(src,'r') as f:
            yamls = yaml.load_all(f,Loader=yaml.SafeLoader)
            for y in yamls:
                return y
    except:
        warning('Invalid YAML block in ' + str(src))
        return {}

# ---
# FOLDER FUNCTIONS
# ---

# Return the full source folder containing item
# - item: relative path to the item
def src_folder_of(item: Path) -> Path:
    return src_root / item.parent # full source folder of item

# Return the full source folder pathname pointing to item
# - item: relative path to the item
def src_path_of(item: Path) -> Path:
    return src_root / item

# Return the relative path (wrt src_root) for the full path leading to an item
# -item: full path in source tree to item
def item_relpath(item: Path) -> Path:
    return item.relative_to(src_root)

# Return the full destination folder to store the result for item
# - item: relative path to the item
def dest_folder_of(item: Path) -> Path:
    return dest_root / item.parent # full source folder of item

# Return the full destination path for the html file to be generated
# for the item
# - item: relative path to the item
def dest_of (item: Path) -> Path:
    dest = dest_root / item
    return dest.with_suffix('.html')

# Return an iterator that iterates over all files in the tree rooted at
# src_root with the specified suffix
# - suffix: suffix of files to return, including "."
def src_files_with_suffix(suffix: str):
    return Path(src_root).glob('**/*' + suffix)


# Determine list of Paths containing the comments (empty if none) for the item
# I.e. all files matching <src_root> / <parent_folder> / *.mdc
# - item: relative path to the item
def commentfiles_for (item: Path):
    return (src_root / item.parent).glob('*.mdc')

# Return path pointing to the root for the current item on the hosting site.
# This is either a relative path back to the root from the path containing the
# item, or simply hosting_root if set to non-empty on the command line 
# - item: relative path to item
def item_root (item: Path) -> str: 
    if hosting_root != '':
        return hosting_root
    else:
        item = str(item)
        log(5,'Computing root for ' + item)
        res = ''
        for c in item:
            if c == '/':
                res = res + '../'
        # remove trailing backslash (does nothing when res == '')
        res = res[:-1]
        if res == '':
            res = '.'
        log(5,'Computed root ' + res)    
        return res

# Determine the link pointing to the item on the hosting site
# relative to context, i.e.
# the generated link works if is included in the page with path context
# - item: relative path to item
# - context: relative path to page that will contain the link to item 
def item_url (item: Path, context: Path) -> str:
    item = item.with_suffix('.html')
    return str(item_root(context) / item)

# Return the full path for a template, throw error if it does not exist
# - name: template file name to be found in the template directory  
def get_template(name: Path):
    template = templates / name        
    if not template.exists():
        error("Template %s not found" % template)
    return template

# ---
# OTHER FUNCTIONS
# ---

# Replace the contents of a file with new contents, but only change the file
# and its metadata when the new contents are different from the current
# contents.
# - current: file whose contents must be updated 
# - update: file containing new content
def replace_when_changed (current: Path, update: Path):
    if (not current.exists()) or \
       (not filecmp.cmp(current,update,shallow=False)): 
        log(5,'File changed ' + str(current))
        update.rename(current)
    else:
        log(5,'File not changed ' + str(current))
        update.unlink() # remove file

# Parse a date if it is a string, return unchanced otherwise.
# (Only parses wordpress generated dates like "Sun, 19 Apr 2020 08:42:00 +0000"
# that the YAML parser fails to parse)
# - s: string to parse if it isn't a date
def parse_date (s) -> datetime.date:
    if type(s) is str:
        time = datetime.datetime.strptime(s,"%a, %d %b %Y %H:%M:%S %z")
        return time.date()
    else:
        return s

# Convert a date to something like "April 03, 2013"
# - date: datetime object to be converted to a string
def normalize_date(date: datetime.date) -> str:
    # we do NOT want zero-padding on days
    return date.strftime("%B ") + str(date.day) + ', ' + str(date.year)

# ---
# PROCESS POSTS
# ---

# Process post metadata and store it in post_metadata[].
# Determine the categories and the tags assiociated with a post 
# and add its  pathname to the necessary categories and tag dictionaries
# - post : full pathname to the post to process
def process_post_metadata(post: Path):
    global items_with_tag
    global items_with_cat
    global post_metadata
    log(2,'Processing categories and tags for ' + str(post))
    yaml = get_yaml_dict(post)
    post_relpath = item_relpath(post)
    if 'categories' in yaml:
        cats = yaml['categories']
        log(3,'Categories: ' + str(cats))
        # careful: categories may be a single string
        if type(cats) is str:
            cats = [ cats ]
        for cat in cats:
            # make sure categoreis are stored in all lowercase
            # (e.g. ios and iOS would otherwise be considered different tags
            # but written to the same external tag file if the filesystem
            # is case-insensitive; one overwrting the other!!!
            cat = cat.lower()
            if cat not in items_with_cat:
                items_with_cat[cat] = []
            items_with_cat[cat].append(post_relpath)
    if 'keywords' in yaml:
        tags = yaml['keywords']
        log(3,'Tags: ' + str(tags))
        # careful: tags may be a single string
        if type(tags) is str:
            tags = [ tags ]
        for tag in tags:
            tag = tag.lower()
            if tag not in items_with_tag:
                items_with_tag[tag] = []
            items_with_tag[tag].append(post_relpath)
    if 'title' in yaml:
        title = yaml['title']
    else:
        title = ''
        warning('Title expected but not found in ' + str(post) + '. Using empty title.')
    if 'date' in yaml: 
        tmp = yaml['date']
        date = parse_date(tmp)
    else:
        date = datetime.date.today()
        warning('Date expected but not found in ' + str(post) + '. Using today.')
    if 'template' in yaml:
        template = yaml['template']
    else:
        template = ''
    post_metadata[post_relpath] = (date,title,template)

# Regular expression matching a path for a post (i.e. one that starts with a
# four digit year)
post_re = re.compile('[0-9][0-9][0-9][0-9].*')
    
# Determine whether the item is a post (or a page)
# - item: relative path to item to determine type of
def is_post(item: Path) -> bool:
    return (post_re.match(str(item)) != None) and (item.suffix == '.md')


# Collect all metadata about all posts (like tags and categories, but also
# title and date for all blog entries) in the source tree. This initializes
# the following global variables:
# items_with_cat, items_with_tag, post_metadata and all_posts
def process_posts ():
    log(0,'Processing posts')
    # Find all posts (they have suffix .md) and process their metadata
    for item in src_files_with_suffix('.md'):
        if is_post(item_relpath(item)):
            process_post_metadata(item)
    # sort all posts by date and store the list of relative paths in all_posts
    for post in post_metadata:
        all_posts.append(post)
    # sort in decending order
    all_posts.sort(reverse=True,key=lambda entry: post_metadata[entry][0])
    
        

# ---
# LaTeX EQUATIONS
# ---

# number of matches so far
eq_match_count = 0
# folder containing the currently processed file
basefolder = ''
# dictonary of processed equations; reset for every file
equations = {}

# Turn a latex expression into an SVG graphic and write it to file.
# The filename to use is determined by the global variable basefolder
# (derived from the name of the file containing the equations) and
# number of processed equations for that file
# - latex_expr: latex expression (without surrounding $$)
# - returns: the name of the SVG file in which the graphic is stored
#   (relative to basename)
def process_eq(latex_expr):
    global eq_match_count
    eq_match_count = eq_match_count + 1
    # create the SVG output name
    svg_output_name = 'index.' + str(eq_match_count) + '.svg'
    svg_full_path = basefolder / svg_output_name
    log(4,'Storing equation in ' + str(svg_full_path))
    # creata a temorary direcotry to work in; will be deleted when with closes
    with tempfile.TemporaryDirectory() as tmpdir:
        tmp_tex = tmpdir + '/htsv.tex'    
        tmp_pdf = tmpdir + '/htsv.pdf'
        # read the latex template and store it as a string in t
        eq_template = get_template('ssst-eq.tex')
        with open(eq_template,'r') as f:
            latex_template = f.read()
        # insert latex_expr at $$ in the template
        s = latex_template.split('$$')
        latex_document = s[0] + '$'+ latex_expr + '$' + s[1]
        # write the result to a tex file in the temporary directory
        with open(tmp_tex,'w') as w:
            w.write(latex_document)
        # create the pdf (in the temporary directory) from the latexfile
        run_shell_command('pdflatex -output-directory ' + \
                             tmpdir + ' ' + tmp_tex)
        # create the SVG from the pdf
        run_shell_command('pdf2svg ' + tmp_pdf + ' ' + str(svg_full_path))
    return svg_output_name

# Match &lt;
lessthan_re = re.compile(r'(&lt;)')

# Match &gt;
greaterthan_re = re.compile(r'(&gt;)')

# Replace any occurance of '&lt;' on the input with '<'
# (because the --gladtex option of pandoc turns < into &lt;)
def sanitize_eq (expr):
    ret = lessthan_re.sub('<',expr,count=0)
    ret = greaterthan_re.sub('>',ret,count=0)    
    log(5,'Sanitized '+ expr + ' to ' + ret)
    return ret

# Match a simple expression, that can be passed without processing
simple_expr_re =  re.compile(r'([A-Za-z0-9])+')

# Process to currently matched latex equation to create an SVG for it
# return the replacement text for the currently matched latex equation
# (ie an <img> tag pointing the SVG file generated for it)
# If the exact same equation was already made, immediately return the
# <img> tag pointing to the SVG file previously made.
# - matchobj: object found to match the equation regular expression 
def substitute_eq (matchobj):
    if matchobj:
        # get the matched latex expression string
        match = matchobj.group(1)
        # sanitize expressions
        latex_expr = sanitize_eq(match)
        log(4,'Processing equation ' + latex_expr)
        # do not process simple LaTex equations, when so instructed
        if keep_simple_eqs and simple_expr_re.fullmatch(latex_expr):
            return '<em>' + latex_expr + '</em>'
        if latex_expr in equations:
            log(5,'Already made!')
            svg_output_name = equations[latex_expr]
        else:
            svg_output_name = process_eq(latex_expr)
            equations[latex_expr]=svg_output_name
        # return the <img> tag that points to the generated SVG
        return '<img class="latexeq"' \
            + ' src="' + svg_output_name + '"' \
            + ' alt="' + latex_expr + '">'
    
# Match <eq ...>LATEXEXPRESSION</eq> generated by pandoc -gladtex
# (use .*? for non-greedy matching, to ensure the first </eq> found
# will close the regulear expression)
gladtex_eq_re = re.compile(r'<eq[^>]*>(.*?)</eq>',re.DOTALL)

# Match <span class="math inline">$LATEXEXPRESSION$</span> generated by
# pandoc without any math processing option.
# (use .*? for non-greedy matching, to ensure the first $</span> found
# will close the regulear expression)
plain_eq_re = re.compile(r'<span class="math inline">\$(.*?)\$</span>',re.DOTALL)

# Process any equations in the item (determined using the regular
# expression eq_re)
# - item: relative path to the post to be processed
def process_equations (item):
    global eq_match_count
    global basefolder
    global equations
    log(2,'Processing equations')
    eq_match_count = 0
    equations = {}
    fname = dest_of(item)
    basefolder = fname.parent
    # open the file for reading
    with open(fname,'r') as f:
        # read the file as one string
        orig_html = f.read()
    # process and substitute each occurence of a latex equation
    if use_gladtex:
        processed_html = gladtex_eq_re.sub(substitute_eq, orig_html, count=0)
    else:
        processed_html = plain_eq_re.sub(substitute_eq, orig_html, count=0)
    # write the result back to the file
    with open(fname,'w') as f:
        f.write(processed_html)

# ---
# MAKE POST / PAGE
# ---

# return the path to next post (more recent) following item; '' if none
# (this uses the fact that all_posts is sorted by date in descending order)
def next_post(item):
    if item in all_posts:
        pos = all_posts.index(item)
        if pos-1 >= 0:
            return all_posts[pos-1]
    return ''

# return the path to previous post (less recent) following item; '' if none
# (this uses the fact that all_posts is sorted by date in descending order)
def prev_post(item):
    if item in all_posts:
        pos = all_posts.index(item)
        if pos+1 < len(all_posts):
            return all_posts[pos+1]
    return ''

# Make the html item from the md or sst source specified by item, merging
# in any comments, and using item_template as the template
def make_item(item,tmpdir,item_template):
    # create destination folder if it doesnt exist yet
    os.makedirs(dest_folder_of(item), exist_ok=True)
    # blog-post path for reply links: strip the source prefix in front
    # and the index.md at the end; add comment.
    reply_path = str(item.parent) + '/comment.'
    # create metadata for next and prev links when relevant
    extra_metadata_str = ''
    next = next_post(item)
    if next != '':
        extra_metadata_str = ' --metadata next="' + item_url(next,item) + '"'
    prev = prev_post(item)
    if prev != '':
        extra_metadata_str = extra_metadata_str + \
            ' --metadata prev="' + item_url(prev,item) + '"'
    # add normalised date metadata for posts (archives and tag/category indices
    # do  not have this, but are treated as posts for the time being)
    if item in post_metadata:
        date = post_metadata[item][0]
        extra_metadata_str = extra_metadata_str + \
            ' --metadata date="' + normalize_date(date) + '"' 
    # build the command string
    src = src_path_of(item)
    dest = dest_of(item)
    dest_tmp = dest.with_suffix('.tmp')
    if use_gladtex:
        gladtex_str = ' --gladtex '
    else:
        gladtex_str = '' 
    command = 'pandoc ' + gladtex_str + '-s -f markdown -t html --template=' + \
        str(item_template) + \
        ' -A '+ tmpdir + '/comments.html' + \
        ' --metadata reply=' + reply_path + \
        ' --metadata root=' + str(item_root(item)) + \
        extra_metadata_str + \
        ' -o '+ str(dest_tmp) + ' ' + str(src)
    run_shell_command(command)
    replace_when_changed(dest,dest_tmp)    

# Return the template to use to process item: default if none specified
# in the YAML header for item
def item_template(item,default):
    if item in post_metadata:
        template = post_metadata[item][2]
        if template != '':
            return get_template(template)
    return get_template(default)
    

# Make the html post from the md source specified by item, merging
# in any comments
def make_post(post,tmpdir):
    log(2,'Processing post')
    post_template = item_template(post,'ssst-post.html')
    make_item(post,tmpdir,post_template)
    
# Make the html post from the md source specified by item, merging
# in any comments
def make_page(page,tmpdir):
    log(2,'Processing page')
    if str(page) == 'archives.sst':
        page_template = get_template('ssst-archives.html')
    else:
        page_template = item_template(page,'ssst-page.html')
    make_item(page,tmpdir,page_template)

# ---
# MAKE COMMENTS
# ---

# Assume all comment files have been converted to html and are stored in
# the temporary directory. 
# Recursively fold all coments at the current level (specified by path in
# the temporary directory)
# and return them as a string; try: <path>.1.html (and fold its children)
# then  <path>.2
def fold_comments(path):
    log(4,'Folding ' + str(path)) 
    folded = ''
    i = 1
    while True:
        prefix = path + '.' + str(i)
        filename = prefix + '.html'
        if not os.path.exists(filename):
            break
        with open(filename,'r') as f:
            folded = folded \
                + '<div class="comment">\n'\
                + f.read() \
                + fold_comments(prefix) \
                + '</div>\n'
        i = i + 1
    return folded
    
# Process the comments associated with the item. 
# - first convert individual comments to html in tmpdir
# - then fold them into one html file (keeping the hierarchical order)
# Stores the result in tmpdir/comments.html for further processing
# by make post (this file is empty if there are no comments)
def make_comments(item,tmpdir):
    log(2,'Processing comments')
    commentfiles = commentfiles_for(item)
    comment_template = get_template('ssst-comment.html')
    # first convert all .mdc files in src_root to .html files in tmpdir
    for c in commentfiles:
        # filename to store html converted comment in (in tmpdir)
        outf = tmpdir + '/' + str(c.name)[:-3] + 'html'
        # blog-post path and comment depth indicator for reply links
        # strip the source prefix in front and mdc from the back
        reply_path = str(item_relpath(c))
        reply_path = reply_path[:-3]
        log(3,'Making comment HTML ' + outf)
        command = 'pandoc -f markdown'+ \
            ' --metadata title=None'  + \
            ' --metadata reply=' + reply_path + \
            ' --template=' + str(comment_template) + \
            ' -o ' + outf + ' ' + str(c)
        run_shell_command(command)
    # now recursively fold in comments created in tmpdir
    comments = fold_comments(tmpdir + '/comment')
    log(5,comments)
    # and store the comments in commetns.html in the tmpdir
    with open(tmpdir + '/comments.html','w') as f:
        f.write(comments)

# ---
# DUMP CATS AND TAGS ; ARCHIVES
# ---

# Write a link (entry_url) describing an entry (using its title and date)
# to the output stream outf
def dump_entry(entry,outname,outf):
    (date,title,template) = post_metadata[entry]
    datestr = normalize_date(date)
    outf.write('- [' + title + '](' + item_url(entry,outname) + \
               ') (' + datestr + ')\n')    
    
# Create a markdown document into out_name (relative path)
# (with title as title and label as subtitle)
# containing a list of links to all posts in filelist
# (assumed to be sorted by date)
def dump(out_name,type,label,filelist):
    # sort the list of pathnames by date, most recent first
    # (using the fact that the path encodes the file date)
    filelist.sort(reverse=True)
    os.makedirs(src_folder_of(out_name),exist_ok=True)
    log(3,'Dumping to ' + str(out_name))
    with DelayedTextfile(src_path_of(out_name)) as outf:
        outf.write("---\n")
        outf.write("title: '" + label.capitalize() + "'\n")
        outf.write("subtitle: '" + type + "'\n")        
        outf.write("---\n")        
        for entry in filelist:
            dump_entry(entry,out_name,outf)
                
# Dump the category and tag dictionaries to the necessary pages in the
# ./category/.. and ./tag/... directories in the src_root (SOURCE!) tree;
# These are markdown files (with suffix .sst) that will subsequently be
# converted to html through the make routine
def dump_cats_and_tags():
    log(0,'Dumping category and tag files.')
    for cat in items_with_cat:
        dump(Path('category/' + cat + '/index.sst'),
             'Category',cat,items_with_cat[cat])
    for tag in items_with_tag:
        dump(Path('tag/' + tag + '/index.sst'),
             'Tag',tag,items_with_tag[tag])

# return a sort key for an entry such that entries are sorted
# by decreasing year, but increasing month and day
def archive_sort_key(entry):
    date = post_metadata[entry][0]
    return (-date.year,date.month,date.day)

# Create archive files containing all posts for a particular month in 
# ./yyyy/mm/index.sst in the src_root (SOURCE!) tree;
# Also create a list of all known archives in src_root/archives.sst
# These are markdown files that will subsequently be converted to html
# through the make routine
def dump_archives():
    log(0,'Dumping archives.')
    last_year_archived = 0
    # dictionary with DelayedTextFile descriptors, indexed by archivepath
    archive_paths = {}
    # sort the posts by *decreasing* year, but increasing month/day within
    # a year; this ensures that the main archive file lists the most
    # recent year first
    archive_posts = all_posts.copy()
    archive_posts.sort(key=archive_sort_key)
    # create the root archive file (as a DelayedTextfile
    archive = src_path_of('archives.sst')
    log(3,'Writing all archives to ' + str(archive))
    # DelayedTextfile will be written to disk when with closes
    # (and contents really chagned)
    with DelayedTextfile(archive) as archivef:        
        archivef.write("---\n")
        archivef.write("title: 'Archives'\n")
#        archivef.write("template: 'ssst-archives.html'\n")
        archivef.write("---\n")
        # process all posts and put them in the appropriate archive
        # this creates another DelayedTextfile for every new archive
        for post in archive_posts:
            # construct the full and relative path for the archive this post
            # should appear in
            date = post_metadata[post][0]
            archive_year = date.year
            monthyear = date.strftime("%B %Y")
            archive_relative_path = date.strftime("%Y/%m/index.sst")
            archive_path = src_path_of(archive_relative_path)
            # if we are processing a new month
            if archive_path not in archive_paths:
                # write a header every time the year changes to the main
                # archive file (this assumes archive_posts is sorted)
                if last_year_archived != archive_year:
                    last_year_archived = archive_year
                    archivef.write('\n## ' + str(archive_year) + '\n')
                # write a link to this new archive to the main archive file
                archivef.write('- [' + monthyear +'](' + \
                               item_url(Path(archive_relative_path),Path('archives.sst')) + ')\n' )
                # create a new delayed file for this monthly archive
                log(3,'Creating new monthly archive ' + str(archive_path))
                outf = DelayedTextfile(archive_path)
                archive_paths[archive_path] = outf
                # write the header for the new archive
                outf.write("---\n")
                outf.write("title: 'Archive " + monthyear + "'\n")        
                outf.write("---\n")
            # append the entry to the monthly archive
            outf = archive_paths[archive_path]
            dump_entry(post,archive_relative_path,outf)
    # close all archive files
    for archive_path in archive_paths:
        archive_paths[archive_path].close()
    
# ---    
# DETERMINE WORKLIST
# ---

# Minimum amount of seconds difference between destination and source m
# modification time before an item will be considered a workitem
grace_period = 60
    
# Determine whether the item (given src and dest directories in the global
# variables src_root and dest_root) should be remade: i.e. if the destination
# does not exist, or when it is older than the source file or any of its
# associated comments (within a certain grace period)
# - item: path to item
# - returns: whether item is a workitem or not
def is_workitem(item: Path) -> bool:
    # check whether destination exists
    dest = dest_of(item)
    log(4,'Destination should be ' + str(dest))
    if not dest.exists():
        return True
    log(4,'Destination exists.')
    # check whetehr destination is older than source
    src = src_path_of(item)
    src_time = os.path.getmtime(src)
    dest_time = os.path.getmtime(dest)
    if (dest_time <= src_time - grace_period) :
        log(4,'Destination older than source.')
        return True
    # check whether destination is older than any comment in the source
    comments = commentfiles_for(item)
    for comment in comments:
        log(4,'Comment found: ' + str(comment))
        comment_time = os.path.getmtime(comment)
        if (dest_time <= comment_time - grace_period):
            log(4,'Destination older than comment in source.')
            return True
    return False


# match all files with suffix in the tree rooted at src_root and return a
# list of all files that need to be (re)made
# - suffix: suffix of files to consider (.md or .sst)
# - returns: a list of paths to items that need to be made
def worklist_for (suffix: str):
    list = []
    # traverse all files with suffix in the subtree rooted at src
    # (This returns a 'list' of Paths (not strings))
    for src in src_files_with_suffix(suffix):
        log(2,'Considering: ' + str(src))
        # strip the src_root
        item = item_relpath(src)
        # add the item when -f given, or when a workitem
        if force or is_workitem(item):
            log(2,'Adding ' + str(src))
            list.append(item)
        else:
            log(2,'Ignoring ' +  str(src))
    return list

# Determine posts, pages and generated pages that need to be (re)made
# - returns: a list of paths to items that need to be made
def worklist ():
    log(0,'Determining files to process...')
    # determine all .md files that need to be made
    list = worklist_for('.md')
    extended_list = list.copy()
    # insert any "neighbours" of posts in the worklist as well, to ensure
    # that their next and prev links are updated
    for post in list:
        # if post is not a post really (but a page), prev=''
        prev = prev_post(post)
        if (prev != '') and (prev not in extended_list):
            extended_list.append(prev)
        next = next_post(post)
        if (next != '') and (next not in extended_list):
            extended_list.append(next)
    # add all *.sst files that need to be made
    final_list = extended_list + worklist_for('.sst')
    log(0,str(len(final_list)) + ' files to process found')
    return final_list

# ---
# MAKE
# ---

# Suffix of media files that will automatically be copied from the source
# folder to the destination folder
media_types = ['jpg','gif','pdf','png']

# Copy all media associated with an item, keeping their metadata.
# - item: path to item in the source tree
def copy_media(item: Path):
    log(2,'Copying mediafiles')
    src_folder = src_folder_of(item)
    dest_folder = dest_folder_of(item)
    for t in media_types:
        for f in src_folder.glob('*.' + t):
            log(4,'Copying media file ' + str(f) + ' to ' + str(dest_folder))
            shutil.copy2(f,dest_folder) # keep metadata

# Remove all non html files (not diretories) from a folder
# (Keep the html file because that will be remade, and we want to check
# later whether the new version is the same as the old or not)
# - folder: folder to clean
def clean_folder(folder: Path):
    log(2,'Cleaning destination ' + str(folder))
    for f in folder.glob('*'):
        if (not f.is_dir()) and (f.suffix != '.html'):
            os.remove(f)

# Make the output (in dest_root) for the item (from src_root)
def make(item: Path):
    log(1,'Processing ' + str(src_path_of(item)))
    # Clean the destination folder
    dest_folder = dest_folder_of(item)
    if dest_folder.exists():
        clean_folder(dest_folder)
    # creata a temorary direcotry to work in; will be deleted when with closes
    with tempfile.TemporaryDirectory() as tmpdir:
        make_comments(item,tmpdir)
        if is_post(item):
            make_post(item,tmpdir)
        else:
            make_page(item,tmpdir)
    process_equations(item)
    copy_media(item)

# ---
# MAKE HOME
# ---

# Regular expression matching img src's and href's that point to a local
# media object (i.e. paths that start with './' and then do not contain
# further slashes). A match returns two groups: 1) the src/href prefix and
# 2) the path without the leading "."
# (FRAGILE CODE: OTHER ITEMS MAY ALSO START WITH ./ )
localpath_re = re.compile(r'(src="|href=")\.(/[^/ ]+)"')

# Replacement to use when replacing local paths 
localpath_replacement = '' 

# filter replacing matched localpath with replacement text
# (using localpath_replacement)
# - matchobj: current match
def substitute_localpath(matchobj) -> str:
    if matchobj:
        # get the matched latex expression string
        prefix = matchobj.group(1)
        name = matchobj.group(2)        
        return(prefix + localpath_replacement + name + '"')


# Summarasise a post and write the summary to a file. Summary is generated using
# pandoc with <templates>/ssst-summary-entry.html as template. (Note: make
# sure that this template does NOT generate a standalone HTML document; it is
# included in another page).
# - post: post to summarise
# - fname: name of file to store summery in
def summarize_post(post: Path, fname: str):
    global localpath_replacement
    date = post_metadata[post][0]
    # create a URL for the  post to be summarised
    post_url = item_url(post,Path('index.html'))
    post_path = src_path_of(post)                    
    summary_entry_template = get_template('ssst-summary-entry.html')
    # set the pandoc command string
    command = 'head -n ' + str(summary_length) + ' ' + str(post_path) + \
            ' | pandoc -s -f markdown --template=' + str(summary_entry_template) + \
        ' --metadata entrydate="' + normalize_date(date) + '"' + \
        ' --metadata root="' + str(item_root(Path('index.html'))) + '"' + \
        ' --metadata path="' + post_url + '"' + \
        ' -o '+ str(fname)
    run_shell_command(command)
    # fix any local links in the output (that point to local media in the
    # directory containing the post) by replacing them with their full path
    # from the root of the destination tree (where the home page is where this
    # summary will be included)
    # FIXME: this assumes that summaries are only included in the home page
    localpath_replacement = str(post.parent)
    with open(fname,'r') as f:
        # read the file as one string
        orig_html = f.read()
    processed_html = localpath_re.sub(substitute_localpath, orig_html, count=0)
    # write the result back to the file
    with open(fname,'w') as f:
        f.write(processed_html)

    
# Make the home page, containing a summary of the five most revent posts
# Home page is generated using pandoc, with <templates>/ssst-homepage.html
# as template and <src>/index.sst as markdown input
def make_home():
    log(0,'Making home page.')
    # get the 5 most recent posts 
    recent_posts = all_posts[0:5]
    # creata a temorary direcotry to work in; will be deleted when with closes
    with tempfile.TemporaryDirectory() as tmpdir:
        count = 0
        # string keeping track of which temporary summary file to include later
        pandoc_append_str = ''
        for post in recent_posts:
            # create a temporary html file summarizing this entry
            count = count + 1
            summary_name = tmpdir + '/summary.' + str(count) + '.html'
            summarize_post(post,summary_name)
            # and add this to the files to append when making the home page
            pandoc_append_str = pandoc_append_str + ' -A ' + summary_name 
        # create the home page, including the summarized entries    
        # any content in <src>/index.sst will be prepended before the summarised
        # posts; in particular its metadata determine the title and subtitle
        summary_template = get_template('ssst-homepage.html')
        src = src_path_of('index.sst')
        if not src.exists():
          error('Home page input %s does not exist.' % src)
        dest = dest_of('index.html')
        dest_tmp = dest.with_suffix('.tmp')
        # set the pandoc command string
        command = 'pandoc -s -f markdown -t html --template=' + \
            str(summary_template) + \
            pandoc_append_str + \
            ' --metadata root=' + str(item_root(Path('index.html'))) + \
            ' -o '+ str(dest_tmp) + ' ' + str(src)
        run_shell_command(command)
        # replace file containing the home page only if its contents changed
        replace_when_changed(dest,dest_tmp)    

# ---
# MAIN
# ---

# Set up arguments parsing
parser = argparse.ArgumentParser()
parser.add_argument('-s', '--source', default=src_root,
                    help='Path to the root of the source files tree')
parser.add_argument('-d', '--destination', default=dest_root,
                    help='Path to the root of the destination files tree')
parser.add_argument('-r', '--root', default=hosting_root,
                    help='Root where the site is hosted. If omitted, the root is found using relative addressing, making the site relocatable')
parser.add_argument('-t', '--templates', default=templates,
                    help='Path to directory with templates')
parser.add_argument('-v', '--verbosity', default=loglevel,
                    help='Verbosity (-1, the default, is silent)')
parser.add_argument('-l', '--logfile', default='ssst.log',
                    help='Name of file to store all log messages in')
parser.add_argument('-f', '--force', default = False, action='store_true',
                    help='(Re)make everything')
parser.add_argument('-g', '--gladtex', default = False, action='store_true',
                    help='Use pandoc --gladtex to process LaTeX equations.')
parser.add_argument('-k', '--keepsimple', default = False, action='store_true',
                    help='Do not process simple LaTex equations')
parser.add_argument('-p', '--pedantic', default = False, action='store_true',
                    help='Abort after warning')
parser.add_argument('-z', '--summarylength', default=summary_length,
                    help='Number of lines in post (including YAML header) to include in a summary')

args = parser.parse_args()

# Apply arguments and check validity
src_root = Path(args.source)
if not src_root.exists():
    error('Source tree %s does not exist.' % src_root)
dest_root = Path(args.destination)
if not dest_root.exists():
    error('Path to destination tree %s does not exist.' % dest_root)
hosting_root = args.root
templates = Path(args.templates)
if not templates.exists():
    error('Templates directory %s does not exist.' % templates)
loglevel = int(args.verbosity)
force = args.force
use_gladtex = args.gladtex
pedantic = args.pedantic
keep_simple_eqs = args.keepsimple
logname = args.logfile
summary_length = int(args.summarylength)

# open logfile
if logname != '':
    logfile = open(logname,'w')

# first collect all metadate (like tags and categories, but also title and date
# for all blog entries in the source tree. This initializes
# - items_with_cat
# - items_with_tag
# - post_metadata
# - all_posts (essentially the tags of post_metadata, but sorted by date)
process_posts()

# dump the category and tag files in ./category/<categorystring>/index.sst and
# ./tag/<tagstring>/index.sst
dump_cats_and_tags()

# dump all archive files ./yyyy/mm/index.sst and ./archives.sst
dump_archives()

# make all html files
for item in worklist():
    make(item)

# create the home page containing a summary of the most recent posts
make_home()

# close the logfile if necessary
if logfile != None:
    logfile.close()


